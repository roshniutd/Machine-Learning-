{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# <font color='Black'> Project 2 Regression"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 252,
   "metadata": {},
   "outputs": [],
   "source": [
    "#importing libraries\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 253,
   "metadata": {},
   "outputs": [],
   "source": [
    "#reading data\n",
    "df=pd.read_csv('avocado2.csv')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 254,
   "metadata": {},
   "outputs": [],
   "source": [
    "df=df.sample(frac=.1,random_state=0)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Data Exploration and Preprocessing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 256,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Sl.No</th>\n",
       "      <th>Date</th>\n",
       "      <th>AveragePrice</th>\n",
       "      <th>Total Volume</th>\n",
       "      <th>4046</th>\n",
       "      <th>4225</th>\n",
       "      <th>4770</th>\n",
       "      <th>Total Bags</th>\n",
       "      <th>Small Bags</th>\n",
       "      <th>Large Bags</th>\n",
       "      <th>XLarge Bags</th>\n",
       "      <th>Type</th>\n",
       "      <th>Year</th>\n",
       "      <th>Region</th>\n",
       "      <th>Month</th>\n",
       "      <th>Season</th>\n",
       "      <th>Rain</th>\n",
       "      <th>Snow</th>\n",
       "      <th>Month Number</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>9181</th>\n",
       "      <td>9181</td>\n",
       "      <td>12/6/2015</td>\n",
       "      <td>1.48</td>\n",
       "      <td>4400.25</td>\n",
       "      <td>1358.53</td>\n",
       "      <td>1735.98</td>\n",
       "      <td>0.00</td>\n",
       "      <td>1305.74</td>\n",
       "      <td>130.00</td>\n",
       "      <td>1175.74</td>\n",
       "      <td>NaN</td>\n",
       "      <td>organic</td>\n",
       "      <td>2015</td>\n",
       "      <td>Atlanta</td>\n",
       "      <td>Dec</td>\n",
       "      <td>Winter</td>\n",
       "      <td>No</td>\n",
       "      <td>Yes</td>\n",
       "      <td>12</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1013</th>\n",
       "      <td>1013</td>\n",
       "      <td>7/5/2015</td>\n",
       "      <td>NaN</td>\n",
       "      <td>190716.43</td>\n",
       "      <td>4890.33</td>\n",
       "      <td>119457.27</td>\n",
       "      <td>13495.86</td>\n",
       "      <td>52872.97</td>\n",
       "      <td>30631.37</td>\n",
       "      <td>21037.53</td>\n",
       "      <td>1204.07</td>\n",
       "      <td>conventional</td>\n",
       "      <td>2015</td>\n",
       "      <td>Indianapolis</td>\n",
       "      <td>Jul</td>\n",
       "      <td>Fall</td>\n",
       "      <td>Yes</td>\n",
       "      <td>No</td>\n",
       "      <td>7</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>14625</th>\n",
       "      <td>14625</td>\n",
       "      <td>3/20/2016</td>\n",
       "      <td>1.27</td>\n",
       "      <td>1045450.41</td>\n",
       "      <td>105069.07</td>\n",
       "      <td>352698.21</td>\n",
       "      <td>9425.64</td>\n",
       "      <td>578257.49</td>\n",
       "      <td>252881.52</td>\n",
       "      <td>325375.97</td>\n",
       "      <td>NaN</td>\n",
       "      <td>organic</td>\n",
       "      <td>2016</td>\n",
       "      <td>TotalUS</td>\n",
       "      <td>Mar</td>\n",
       "      <td>Spring</td>\n",
       "      <td>No</td>\n",
       "      <td>No</td>\n",
       "      <td>3</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>15234</th>\n",
       "      <td>15234</td>\n",
       "      <td>9/10/2017</td>\n",
       "      <td>2.15</td>\n",
       "      <td>9883.59</td>\n",
       "      <td>313.75</td>\n",
       "      <td>4230.58</td>\n",
       "      <td>0.00</td>\n",
       "      <td>5339.26</td>\n",
       "      <td>2166.91</td>\n",
       "      <td>3172.35</td>\n",
       "      <td>NaN</td>\n",
       "      <td>organic</td>\n",
       "      <td>2017</td>\n",
       "      <td>CincinnatiDayton</td>\n",
       "      <td>Sep</td>\n",
       "      <td>Fall</td>\n",
       "      <td>Yes</td>\n",
       "      <td>No</td>\n",
       "      <td>9</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>18247</th>\n",
       "      <td>18247</td>\n",
       "      <td>1/14/2018</td>\n",
       "      <td>1.93</td>\n",
       "      <td>16205.22</td>\n",
       "      <td>1527.63</td>\n",
       "      <td>2981.04</td>\n",
       "      <td>727.01</td>\n",
       "      <td>10969.54</td>\n",
       "      <td>10919.54</td>\n",
       "      <td>50.00</td>\n",
       "      <td>NaN</td>\n",
       "      <td>organic</td>\n",
       "      <td>2018</td>\n",
       "      <td>WestTexNewMexico</td>\n",
       "      <td>Jan</td>\n",
       "      <td>Spring</td>\n",
       "      <td>No</td>\n",
       "      <td>No</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "       Sl.No       Date  AveragePrice  Total Volume       4046       4225  \\\n",
       "9181    9181  12/6/2015          1.48       4400.25    1358.53    1735.98   \n",
       "1013    1013   7/5/2015           NaN     190716.43    4890.33  119457.27   \n",
       "14625  14625  3/20/2016          1.27    1045450.41  105069.07  352698.21   \n",
       "15234  15234  9/10/2017          2.15       9883.59     313.75    4230.58   \n",
       "18247  18247  1/14/2018          1.93      16205.22    1527.63    2981.04   \n",
       "\n",
       "           4770  Total Bags  Small Bags  Large Bags  XLarge Bags  \\\n",
       "9181       0.00     1305.74      130.00     1175.74          NaN   \n",
       "1013   13495.86    52872.97    30631.37    21037.53      1204.07   \n",
       "14625   9425.64   578257.49   252881.52   325375.97          NaN   \n",
       "15234      0.00     5339.26     2166.91     3172.35          NaN   \n",
       "18247    727.01    10969.54    10919.54       50.00          NaN   \n",
       "\n",
       "               Type  Year            Region Month  Season Rain Snow  \\\n",
       "9181        organic  2015           Atlanta   Dec  Winter   No  Yes   \n",
       "1013   conventional  2015      Indianapolis   Jul    Fall  Yes   No   \n",
       "14625       organic  2016           TotalUS   Mar  Spring   No   No   \n",
       "15234       organic  2017  CincinnatiDayton   Sep    Fall  Yes   No   \n",
       "18247       organic  2018  WestTexNewMexico   Jan  Spring   No   No   \n",
       "\n",
       "       Month Number  \n",
       "9181             12  \n",
       "1013              7  \n",
       "14625             3  \n",
       "15234             9  \n",
       "18247             1  "
      ]
     },
     "execution_count": 256,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 257,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Sl.No</th>\n",
       "      <th>AveragePrice</th>\n",
       "      <th>Total Volume</th>\n",
       "      <th>4046</th>\n",
       "      <th>4225</th>\n",
       "      <th>4770</th>\n",
       "      <th>Total Bags</th>\n",
       "      <th>Small Bags</th>\n",
       "      <th>Large Bags</th>\n",
       "      <th>XLarge Bags</th>\n",
       "      <th>Year</th>\n",
       "      <th>Month Number</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>count</th>\n",
       "      <td>1825.000000</td>\n",
       "      <td>1810.000000</td>\n",
       "      <td>1.825000e+03</td>\n",
       "      <td>1.825000e+03</td>\n",
       "      <td>1.825000e+03</td>\n",
       "      <td>1.825000e+03</td>\n",
       "      <td>1.825000e+03</td>\n",
       "      <td>1.825000e+03</td>\n",
       "      <td>1.825000e+03</td>\n",
       "      <td>620.000000</td>\n",
       "      <td>1825.000000</td>\n",
       "      <td>1825.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>mean</th>\n",
       "      <td>9181.404932</td>\n",
       "      <td>1.417840</td>\n",
       "      <td>8.800425e+05</td>\n",
       "      <td>3.008588e+05</td>\n",
       "      <td>3.009328e+05</td>\n",
       "      <td>2.329346e+04</td>\n",
       "      <td>2.549553e+05</td>\n",
       "      <td>1.932733e+05</td>\n",
       "      <td>5.843724e+04</td>\n",
       "      <td>9550.887774</td>\n",
       "      <td>2016.129863</td>\n",
       "      <td>6.280548</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>std</th>\n",
       "      <td>5223.757500</td>\n",
       "      <td>0.401679</td>\n",
       "      <td>3.407794e+06</td>\n",
       "      <td>1.250146e+06</td>\n",
       "      <td>1.164668e+06</td>\n",
       "      <td>1.045785e+05</td>\n",
       "      <td>9.958174e+05</td>\n",
       "      <td>7.535237e+05</td>\n",
       "      <td>2.490433e+05</td>\n",
       "      <td>28306.466028</td>\n",
       "      <td>0.924870</td>\n",
       "      <td>3.468875</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>min</th>\n",
       "      <td>8.000000</td>\n",
       "      <td>0.530000</td>\n",
       "      <td>5.888700e+02</td>\n",
       "      <td>0.000000e+00</td>\n",
       "      <td>0.000000e+00</td>\n",
       "      <td>0.000000e+00</td>\n",
       "      <td>0.000000e+00</td>\n",
       "      <td>0.000000e+00</td>\n",
       "      <td>0.000000e+00</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>2015.000000</td>\n",
       "      <td>1.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>25%</th>\n",
       "      <td>4693.000000</td>\n",
       "      <td>1.110000</td>\n",
       "      <td>9.267690e+03</td>\n",
       "      <td>8.220800e+02</td>\n",
       "      <td>2.732340e+03</td>\n",
       "      <td>0.000000e+00</td>\n",
       "      <td>4.431280e+03</td>\n",
       "      <td>2.239500e+03</td>\n",
       "      <td>1.225100e+02</td>\n",
       "      <td>123.492500</td>\n",
       "      <td>2015.000000</td>\n",
       "      <td>3.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>50%</th>\n",
       "      <td>9277.000000</td>\n",
       "      <td>1.380000</td>\n",
       "      <td>1.000361e+05</td>\n",
       "      <td>8.217170e+03</td>\n",
       "      <td>2.785819e+04</td>\n",
       "      <td>1.807200e+02</td>\n",
       "      <td>3.745965e+04</td>\n",
       "      <td>2.396951e+04</td>\n",
       "      <td>2.557730e+03</td>\n",
       "      <td>1269.245000</td>\n",
       "      <td>2016.000000</td>\n",
       "      <td>6.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>75%</th>\n",
       "      <td>13591.000000</td>\n",
       "      <td>1.680000</td>\n",
       "      <td>4.317914e+05</td>\n",
       "      <td>1.085584e+05</td>\n",
       "      <td>1.449582e+05</td>\n",
       "      <td>6.909700e+03</td>\n",
       "      <td>1.066111e+05</td>\n",
       "      <td>8.323120e+04</td>\n",
       "      <td>2.380356e+04</td>\n",
       "      <td>5056.632500</td>\n",
       "      <td>2017.000000</td>\n",
       "      <td>9.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>max</th>\n",
       "      <td>18247.000000</td>\n",
       "      <td>2.960000</td>\n",
       "      <td>4.044960e+07</td>\n",
       "      <td>1.550361e+07</td>\n",
       "      <td>1.332657e+07</td>\n",
       "      <td>1.880231e+06</td>\n",
       "      <td>1.489489e+07</td>\n",
       "      <td>1.139283e+07</td>\n",
       "      <td>3.275463e+06</td>\n",
       "      <td>317517.950000</td>\n",
       "      <td>2018.000000</td>\n",
       "      <td>12.000000</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "              Sl.No  AveragePrice  Total Volume          4046          4225  \\\n",
       "count   1825.000000   1810.000000  1.825000e+03  1.825000e+03  1.825000e+03   \n",
       "mean    9181.404932      1.417840  8.800425e+05  3.008588e+05  3.009328e+05   \n",
       "std     5223.757500      0.401679  3.407794e+06  1.250146e+06  1.164668e+06   \n",
       "min        8.000000      0.530000  5.888700e+02  0.000000e+00  0.000000e+00   \n",
       "25%     4693.000000      1.110000  9.267690e+03  8.220800e+02  2.732340e+03   \n",
       "50%     9277.000000      1.380000  1.000361e+05  8.217170e+03  2.785819e+04   \n",
       "75%    13591.000000      1.680000  4.317914e+05  1.085584e+05  1.449582e+05   \n",
       "max    18247.000000      2.960000  4.044960e+07  1.550361e+07  1.332657e+07   \n",
       "\n",
       "               4770    Total Bags    Small Bags    Large Bags    XLarge Bags  \\\n",
       "count  1.825000e+03  1.825000e+03  1.825000e+03  1.825000e+03     620.000000   \n",
       "mean   2.329346e+04  2.549553e+05  1.932733e+05  5.843724e+04    9550.887774   \n",
       "std    1.045785e+05  9.958174e+05  7.535237e+05  2.490433e+05   28306.466028   \n",
       "min    0.000000e+00  0.000000e+00  0.000000e+00  0.000000e+00       1.000000   \n",
       "25%    0.000000e+00  4.431280e+03  2.239500e+03  1.225100e+02     123.492500   \n",
       "50%    1.807200e+02  3.745965e+04  2.396951e+04  2.557730e+03    1269.245000   \n",
       "75%    6.909700e+03  1.066111e+05  8.323120e+04  2.380356e+04    5056.632500   \n",
       "max    1.880231e+06  1.489489e+07  1.139283e+07  3.275463e+06  317517.950000   \n",
       "\n",
       "              Year  Month Number  \n",
       "count  1825.000000   1825.000000  \n",
       "mean   2016.129863      6.280548  \n",
       "std       0.924870      3.468875  \n",
       "min    2015.000000      1.000000  \n",
       "25%    2015.000000      3.000000  \n",
       "50%    2016.000000      6.000000  \n",
       "75%    2017.000000      9.000000  \n",
       "max    2018.000000     12.000000  "
      ]
     },
     "execution_count": 257,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.describe()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 258,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'pandas.core.frame.DataFrame'>\n",
      "Int64Index: 1825 entries, 9181 to 10938\n",
      "Data columns (total 19 columns):\n",
      "Sl.No           1825 non-null int64\n",
      "Date            1825 non-null object\n",
      "AveragePrice    1810 non-null float64\n",
      "Total Volume    1825 non-null float64\n",
      "4046            1825 non-null float64\n",
      "4225            1825 non-null float64\n",
      "4770            1825 non-null float64\n",
      "Total Bags      1825 non-null float64\n",
      "Small Bags      1825 non-null float64\n",
      "Large Bags      1825 non-null float64\n",
      "XLarge Bags     620 non-null float64\n",
      "Type            1825 non-null object\n",
      "Year            1825 non-null int64\n",
      "Region          1825 non-null object\n",
      "Month           1825 non-null object\n",
      "Season          1825 non-null object\n",
      "Rain            1825 non-null object\n",
      "Snow            1825 non-null object\n",
      "Month Number    1825 non-null int64\n",
      "dtypes: float64(9), int64(3), object(7)\n",
      "memory usage: 285.2+ KB\n"
     ]
    }
   ],
   "source": [
    "df.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 259,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<matplotlib.axes._subplots.AxesSubplot at 0x13d1a8855c8>"
      ]
     },
     "execution_count": 259,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 720x576 with 2 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "#finding correlations between numerical features\n",
    "import seaborn as sns\n",
    "df1=df.iloc[:,3:9]\n",
    "f, ax = plt.subplots(figsize=(10, 8))\n",
    "corr = df1.corr()\n",
    "sns.heatmap(corr, mask=np.zeros_like(corr, dtype=np.bool), cmap=sns.diverging_palette(220, 10, as_cmap=True),\n",
    "            square=True, ax=ax)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 260,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Index(['AveragePrice', 'XLarge Bags'], dtype='object')"
      ]
     },
     "execution_count": 260,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#displaying columns having null values\n",
    "df.columns[df.isnull().any()]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 261,
   "metadata": {},
   "outputs": [],
   "source": [
    "#imputing NAN values with mean\n",
    "df['AveragePrice'] = df.groupby('Type')['AveragePrice'].transform(lambda x: x.fillna(x.mean()))\n",
    "df['XLarge Bags'] = df.groupby('Type')['XLarge Bags'].transform(lambda x: x.fillna(x.mean()))\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 262,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Mapping binary variables to 0 and 1 \n",
    "df.Rain = df.Rain.replace({'No' : 0, 'Yes' : 1}) \n",
    "df.Snow = df.Snow.replace({'No' : 0, 'Yes' : 1})\n",
    "df.Type = df.Type.replace({'conventional' : 0, 'organic' : 1}) \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 263,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Replacing values with one hot vector\n",
    "df = pd.get_dummies(df, columns = ['Year'], prefix = ['Year'])\n",
    "df = pd.get_dummies(df, columns = ['Region'], prefix = ['Region'])\n",
    "df = pd.get_dummies(df, columns = ['Month'], prefix = ['Month'])\n",
    "df = pd.get_dummies(df, columns = ['Season'], prefix = ['Season'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 264,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Dropping unnecessary/ redundant columns\n",
    "df.drop(['Date'],axis=1,inplace= True)\n",
    "df.drop(['Sl.No'],axis=1,inplace= True)\n",
    "df.drop(['Total Volume'],axis=1,inplace= True)\n",
    "df.drop(['Total Bags'],axis=1,inplace= True)\n",
    "df.drop(['Month Number'],axis=1,inplace= True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 265,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Creating Feature and target variable\n",
    "y = df['AveragePrice']\n",
    "X = df.drop(['AveragePrice'], axis = 1)\n",
    "names = list(X.columns.values)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 267,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Creating Feature and target variable\n",
    "y = df['AveragePrice']\n",
    "X = df.drop(['AveragePrice'], axis = 1)\n",
    "names = list(X.columns.values)\n",
    "#Scaling & splitting into train and test dataset\n",
    "from sklearn.neighbors import KNeighborsRegressor\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.preprocessing import MinMaxScaler\n",
    "from sklearn.model_selection import train_test_split\n",
    "X_train_org, X_test_org, y_train, y_test = train_test_split(X,y, random_state = 0)\n",
    "scaler = MinMaxScaler()\n",
    "X_train = scaler.fit_transform(X_train_org)\n",
    "X_test = scaler.transform(X_test_org)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Bagging"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 268,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train score 0.6396427418855948\n",
      "Test score 0.587742419732642\n"
     ]
    }
   ],
   "source": [
    "#Decision tree\n",
    "from sklearn.ensemble import BaggingRegressor\n",
    "from sklearn.tree import DecisionTreeRegressor\n",
    "\n",
    "dt_clf =  DecisionTreeRegressor(random_state = 0) \n",
    "bag_clf = BaggingRegressor(dt_clf, n_estimators=500, max_samples=100, bootstrap=True, n_jobs=-1, random_state=0)\n",
    "\n",
    "bag_clf.fit(X_train, y_train)\n",
    "y_pred = bag_clf.predict(X_test)\n",
    "print('Train score',bag_clf.score(X_train, y_train))\n",
    "print('Test score',bag_clf.score(X_test, y_test))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 270,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train score 0.6447499171984035\n",
      "Test score 0.5593015219706792\n"
     ]
    }
   ],
   "source": [
    "#Linear regression\n",
    "from sklearn.ensemble import BaggingRegressor\n",
    "from sklearn.linear_model import LinearRegression\n",
    "\n",
    "dt_clf =  LinearRegression() \n",
    "bag_clf = BaggingRegressor(dt_clf)\n",
    "\n",
    "bag_clf.fit(X_train, y_train)\n",
    "y_pred = bag_clf.predict(X_test)\n",
    "print('Train score',bag_clf.score(X_train, y_train))\n",
    "print('Test score',bag_clf.score(X_test, y_test))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 271,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train score 0.6468286653437065\n",
      "Test score 0.5502650062405123\n"
     ]
    }
   ],
   "source": [
    "#Random forest\n",
    "from sklearn.ensemble import RandomForestRegressor\n",
    "\n",
    "rnd_clf = RandomForestRegressor(n_estimators=500, max_leaf_nodes=16, n_jobs=-1, random_state=0)\n",
    "rnd_clf.fit(X_train, y_train)\n",
    "\n",
    "y_pred_rf = rnd_clf.predict(X_test)\n",
    "print('Train score',rnd_clf.score(X_train, y_train))\n",
    "print('Test score',rnd_clf.score(X_test, y_test))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Pasting"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 272,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train score 0.6473590961524682\n",
      "Test score 0.5946535155538744\n"
     ]
    }
   ],
   "source": [
    "#Decision tree\n",
    "from sklearn.ensemble import BaggingRegressor\n",
    "from sklearn.tree import DecisionTreeRegressor\n",
    "\n",
    "dt_clf =  DecisionTreeRegressor(random_state = 0) \n",
    "bag_clf = BaggingRegressor(dt_clf, n_estimators=500, max_samples=100, bootstrap=False, n_jobs=-1, random_state=0)\n",
    "\n",
    "bag_clf.fit(X_train, y_train)\n",
    "y_pred = bag_clf.predict(X_test)\n",
    "print('Train score',bag_clf.score(X_train, y_train))\n",
    "print('Test score',bag_clf.score(X_test, y_test))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 273,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train score 0.6468944117924181\n",
      "Test score 0.5636503828315553\n"
     ]
    }
   ],
   "source": [
    "#Linear regression\n",
    "from sklearn.ensemble import BaggingRegressor\n",
    "from sklearn.linear_model import LinearRegression\n",
    "\n",
    "dt_clf =  LinearRegression() \n",
    "bag_clf = BaggingRegressor(dt_clf,bootstrap=False)\n",
    "\n",
    "bag_clf.fit(X_train, y_train)\n",
    "y_pred = bag_clf.predict(X_test)\n",
    "print('Train score',bag_clf.score(X_train, y_train))\n",
    "print('Test score',bag_clf.score(X_test, y_test))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 274,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train score 0.5910272344787159\n",
      "Test score 0.4582912689126317\n"
     ]
    }
   ],
   "source": [
    "#Random forest\n",
    "from sklearn.ensemble import RandomForestRegressor\n",
    "\n",
    "rnd_clf = RandomForestRegressor(n_estimators=500, max_leaf_nodes=16, n_jobs=-1,bootstrap=False, random_state=0)\n",
    "rnd_clf.fit(X_train, y_train)\n",
    "\n",
    "y_pred_rf = rnd_clf.predict(X_test)\n",
    "print('Train score',rnd_clf.score(X_train, y_train))\n",
    "print('Test score',rnd_clf.score(X_test, y_test))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Adaboost "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 275,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train score: 0.9996\n",
      "Test score: 0.7253\n"
     ]
    }
   ],
   "source": [
    "from sklearn.ensemble import AdaBoostRegressor\n",
    "\n",
    "ada_clf = AdaBoostRegressor(\n",
    "    DecisionTreeRegressor(), n_estimators=500, learning_rate=0.9, random_state=0)\n",
    "ada_clf.fit(X_train, y_train)\n",
    "print('Train score: {:.4f}'.format(ada_clf.score(X_train, y_train)))\n",
    "print('Test score: {:.4f}'.format(ada_clf.score(X_test, y_test)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 276,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train score: 0.6309\n",
      "Test score: 0.5342\n"
     ]
    }
   ],
   "source": [
    "from sklearn.ensemble import AdaBoostRegressor\n",
    "\n",
    "ada_clf = AdaBoostRegressor(\n",
    "    LinearRegression(), n_estimators=10, learning_rate=0.5, random_state=0)\n",
    "ada_clf.fit(X_train, y_train)\n",
    "print('Train score: {:.4f}'.format(ada_clf.score(X_train, y_train)))\n",
    "print('Test score: {:.4f}'.format(ada_clf.score(X_test, y_test)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Gradient Boosting"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 279,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\users\\sneha liza george\\appdata\\local\\programs\\python\\python37\\lib\\site-packages\\sklearn\\model_selection\\_search.py:823: FutureWarning: The parameter 'iid' is deprecated in 0.22 and will be removed in 0.24.\n",
      "  \"removed in 0.24.\", FutureWarning\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'learning_rate': 0.1, 'n_estimators': 500}\n"
     ]
    }
   ],
   "source": [
    "from  sklearn.ensemble import GradientBoostingRegressor\n",
    "from sklearn.model_selection import GridSearchCV\n",
    "X_train_org, X_test_org, y_train, y_test = train_test_split(X,y, random_state = 0)\n",
    "scaler = MinMaxScaler()\n",
    "X_train = scaler.fit_transform(X_train_org)\n",
    "X_test = scaler.transform(X_test_org)\n",
    "params = {\n",
    "    'n_estimators': [50, 100, 200,500],\n",
    "    'learning_rate':[0.1, 0.25, 0.5, 0.75, 1]\n",
    "    }\n",
    "\n",
    "gc = GradientBoostingRegressor(random_state=0)\n",
    "\n",
    "gc_grid = GridSearchCV(estimator=gc, param_grid=params, cv=5,iid = False)\n",
    "gc_grid.fit(X_train, y_train)\n",
    "best_params = gc_grid.best_params_\n",
    "print(best_params)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 280,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train score: 0.9412\n",
      "Test score: 0.7509\n"
     ]
    }
   ],
   "source": [
    "from  sklearn.ensemble import GradientBoostingRegressor\n",
    "from sklearn.metrics import recall_score\n",
    "\n",
    "\n",
    "gc = GradientBoostingRegressor(random_state=0,learning_rate= 0.1, n_estimators = 500)\n",
    "gc.fit(X_train, y_train)\n",
    "y_pred= gc.predict(X_test)\n",
    "print('Train score: {:.4f}'.format(gc.score(X_train, y_train)))\n",
    "print('Test score: {:.4f}'.format(gc.score(X_test, y_test)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# PCA"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 281,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Creating Feature and target variable\n",
    "y = df['AveragePrice']\n",
    "X = df.drop(['AveragePrice'], axis = 1)\n",
    "names = list(X.columns.values)\n",
    "#Scaling & splitting into train and test dataset\n",
    "from sklearn.neighbors import KNeighborsRegressor\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.preprocessing import MinMaxScaler\n",
    "from sklearn.model_selection import train_test_split\n",
    "X_train_org, X_test_org, y_train, y_test = train_test_split(X,y, random_state = 0)\n",
    "scaler = MinMaxScaler()\n",
    "X_train = scaler.fit_transform(X_train_org)\n",
    "X_test = scaler.transform(X_test_org)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 282,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.decomposition import PCA\n",
    "pca = PCA(n_components=65)\n",
    "principalComponents = pca.fit_transform(x)\n",
    "principalDf = pd.DataFrame(data = principalComponents\n",
    "             , columns = ['principal component 1', 'principal component 2', 'principal component 3',\n",
    "                          'principal component 4', 'principal component 5', 'principal component 6',\n",
    "                          'principal component 7', 'principal component 8', 'principal component 9',\n",
    "                          'principal component 10', 'principal component 11', 'principal component 12',\n",
    "                          'principal component 13', 'principal component 14', 'principal component 15', \n",
    "                          'principal component 16', 'principal component 17', 'principal component 18', \n",
    "                          'principal component 19','principal component 20', 'principal component 21',\n",
    "                          'principal component 22',\n",
    "                          'principal component 23', 'principal component 24', 'principal component 25',\n",
    "                          'principal component 26', 'principal component 27', 'principal component 28',\n",
    "                          'principal component 29', 'principal component 30', 'principal component 31', \n",
    "                          'principal component 32','principal component 33', 'principal component 34', \n",
    "                          'principal component 35',\n",
    "                          'principal component 36', 'principal component 37', 'principal component 38',\n",
    "                          'principal component 39', 'principal component 40', 'principal component 41',\n",
    "                          'principal component 42', 'principal component 43',\n",
    "                          'principal component 44', 'principal component 45', 'principal component 46',\n",
    "                          'principal component 47', 'principal component 48', 'principal component 49','principal component 50'\n",
    "                         ,\n",
    "                         'principal component 51', 'principal component 52', 'principal component 53',\n",
    "                          'principal component 54', 'principal component 55', 'principal component 56',\n",
    "                          'principal component 57', 'principal component 58', 'principal component 59',\n",
    "                          'principal component 60','principal component 61', 'principal component 62', 'principal component 63',\n",
    "                          'principal component 64', 'principal component 65'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 283,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>principal component 1</th>\n",
       "      <th>principal component 2</th>\n",
       "      <th>principal component 3</th>\n",
       "      <th>principal component 4</th>\n",
       "      <th>principal component 5</th>\n",
       "      <th>principal component 6</th>\n",
       "      <th>principal component 7</th>\n",
       "      <th>principal component 8</th>\n",
       "      <th>principal component 9</th>\n",
       "      <th>principal component 10</th>\n",
       "      <th>...</th>\n",
       "      <th>principal component 56</th>\n",
       "      <th>principal component 57</th>\n",
       "      <th>principal component 58</th>\n",
       "      <th>principal component 59</th>\n",
       "      <th>principal component 60</th>\n",
       "      <th>principal component 61</th>\n",
       "      <th>principal component 62</th>\n",
       "      <th>principal component 63</th>\n",
       "      <th>principal component 64</th>\n",
       "      <th>principal component 65</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>-0.960615</td>\n",
       "      <td>2.637565</td>\n",
       "      <td>-1.411701</td>\n",
       "      <td>-0.879872</td>\n",
       "      <td>-1.351151</td>\n",
       "      <td>-1.341275</td>\n",
       "      <td>1.481301</td>\n",
       "      <td>1.936232</td>\n",
       "      <td>-0.093120</td>\n",
       "      <td>0.807181</td>\n",
       "      <td>...</td>\n",
       "      <td>0.289426</td>\n",
       "      <td>-0.089653</td>\n",
       "      <td>-0.898951</td>\n",
       "      <td>0.284482</td>\n",
       "      <td>0.589233</td>\n",
       "      <td>1.837274</td>\n",
       "      <td>-0.503363</td>\n",
       "      <td>0.124744</td>\n",
       "      <td>-0.916952</td>\n",
       "      <td>-2.068852</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>-0.420814</td>\n",
       "      <td>-2.666090</td>\n",
       "      <td>-1.880717</td>\n",
       "      <td>-0.502217</td>\n",
       "      <td>-0.399477</td>\n",
       "      <td>-0.516453</td>\n",
       "      <td>0.209275</td>\n",
       "      <td>0.585650</td>\n",
       "      <td>-2.397758</td>\n",
       "      <td>-0.449515</td>\n",
       "      <td>...</td>\n",
       "      <td>0.228169</td>\n",
       "      <td>-0.791927</td>\n",
       "      <td>1.346568</td>\n",
       "      <td>-0.133753</td>\n",
       "      <td>0.328647</td>\n",
       "      <td>-0.397283</td>\n",
       "      <td>1.479893</td>\n",
       "      <td>-0.508633</td>\n",
       "      <td>1.233658</td>\n",
       "      <td>0.835310</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>2.163969</td>\n",
       "      <td>0.082146</td>\n",
       "      <td>1.819162</td>\n",
       "      <td>-1.228000</td>\n",
       "      <td>-0.341767</td>\n",
       "      <td>1.427034</td>\n",
       "      <td>1.740093</td>\n",
       "      <td>0.836960</td>\n",
       "      <td>0.767482</td>\n",
       "      <td>-0.895097</td>\n",
       "      <td>...</td>\n",
       "      <td>0.391972</td>\n",
       "      <td>0.332891</td>\n",
       "      <td>0.168479</td>\n",
       "      <td>0.257417</td>\n",
       "      <td>-0.250484</td>\n",
       "      <td>1.624782</td>\n",
       "      <td>-0.233683</td>\n",
       "      <td>0.158186</td>\n",
       "      <td>-1.639428</td>\n",
       "      <td>-0.194715</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>-0.750427</td>\n",
       "      <td>-2.950321</td>\n",
       "      <td>-1.883113</td>\n",
       "      <td>-0.501615</td>\n",
       "      <td>0.724095</td>\n",
       "      <td>-0.888319</td>\n",
       "      <td>1.131758</td>\n",
       "      <td>-0.430359</td>\n",
       "      <td>2.837142</td>\n",
       "      <td>0.211283</td>\n",
       "      <td>...</td>\n",
       "      <td>0.545031</td>\n",
       "      <td>0.154213</td>\n",
       "      <td>-1.004270</td>\n",
       "      <td>0.474016</td>\n",
       "      <td>0.080496</td>\n",
       "      <td>-0.605871</td>\n",
       "      <td>0.669171</td>\n",
       "      <td>0.733169</td>\n",
       "      <td>0.058910</td>\n",
       "      <td>-0.379444</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>-0.429275</td>\n",
       "      <td>-0.067060</td>\n",
       "      <td>3.014580</td>\n",
       "      <td>-2.265537</td>\n",
       "      <td>0.520638</td>\n",
       "      <td>-0.044892</td>\n",
       "      <td>1.433443</td>\n",
       "      <td>-1.153129</td>\n",
       "      <td>1.642724</td>\n",
       "      <td>0.464771</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.130639</td>\n",
       "      <td>1.206365</td>\n",
       "      <td>-2.057215</td>\n",
       "      <td>-0.944295</td>\n",
       "      <td>3.539220</td>\n",
       "      <td>-0.574644</td>\n",
       "      <td>-0.472738</td>\n",
       "      <td>-0.983877</td>\n",
       "      <td>-0.294182</td>\n",
       "      <td>-0.533892</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1820</th>\n",
       "      <td>-0.382963</td>\n",
       "      <td>0.120614</td>\n",
       "      <td>0.992047</td>\n",
       "      <td>2.555985</td>\n",
       "      <td>0.911797</td>\n",
       "      <td>-1.354328</td>\n",
       "      <td>0.605447</td>\n",
       "      <td>-1.813473</td>\n",
       "      <td>-2.353026</td>\n",
       "      <td>-0.925162</td>\n",
       "      <td>...</td>\n",
       "      <td>1.378532</td>\n",
       "      <td>0.710236</td>\n",
       "      <td>-0.805937</td>\n",
       "      <td>-1.463426</td>\n",
       "      <td>-1.450543</td>\n",
       "      <td>-1.115401</td>\n",
       "      <td>0.650508</td>\n",
       "      <td>-1.123987</td>\n",
       "      <td>-1.116167</td>\n",
       "      <td>1.409609</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1821</th>\n",
       "      <td>-0.477665</td>\n",
       "      <td>2.834703</td>\n",
       "      <td>-1.989160</td>\n",
       "      <td>-0.635043</td>\n",
       "      <td>1.164131</td>\n",
       "      <td>-0.308848</td>\n",
       "      <td>-1.573901</td>\n",
       "      <td>-0.976429</td>\n",
       "      <td>0.401069</td>\n",
       "      <td>-0.372293</td>\n",
       "      <td>...</td>\n",
       "      <td>0.323943</td>\n",
       "      <td>-1.273216</td>\n",
       "      <td>-0.852462</td>\n",
       "      <td>1.278595</td>\n",
       "      <td>-1.656841</td>\n",
       "      <td>-2.258364</td>\n",
       "      <td>-1.160130</td>\n",
       "      <td>-0.192061</td>\n",
       "      <td>-0.821369</td>\n",
       "      <td>0.449265</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1822</th>\n",
       "      <td>-0.016520</td>\n",
       "      <td>0.108308</td>\n",
       "      <td>1.895882</td>\n",
       "      <td>-1.411357</td>\n",
       "      <td>0.656693</td>\n",
       "      <td>-1.277562</td>\n",
       "      <td>-0.356176</td>\n",
       "      <td>-0.310755</td>\n",
       "      <td>-2.080121</td>\n",
       "      <td>-1.743331</td>\n",
       "      <td>...</td>\n",
       "      <td>0.838994</td>\n",
       "      <td>0.966531</td>\n",
       "      <td>-0.805958</td>\n",
       "      <td>-0.234253</td>\n",
       "      <td>-1.103178</td>\n",
       "      <td>0.253142</td>\n",
       "      <td>0.691452</td>\n",
       "      <td>-1.389206</td>\n",
       "      <td>-0.518220</td>\n",
       "      <td>1.557533</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1823</th>\n",
       "      <td>-0.753309</td>\n",
       "      <td>-3.013575</td>\n",
       "      <td>-1.932386</td>\n",
       "      <td>-0.453725</td>\n",
       "      <td>1.176477</td>\n",
       "      <td>1.475854</td>\n",
       "      <td>0.572913</td>\n",
       "      <td>0.858665</td>\n",
       "      <td>0.104913</td>\n",
       "      <td>0.974299</td>\n",
       "      <td>...</td>\n",
       "      <td>2.106992</td>\n",
       "      <td>0.384889</td>\n",
       "      <td>1.724314</td>\n",
       "      <td>2.206176</td>\n",
       "      <td>-0.079748</td>\n",
       "      <td>-0.889737</td>\n",
       "      <td>0.828298</td>\n",
       "      <td>0.262213</td>\n",
       "      <td>-0.794399</td>\n",
       "      <td>-0.469089</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1824</th>\n",
       "      <td>-0.713694</td>\n",
       "      <td>-0.068698</td>\n",
       "      <td>2.177190</td>\n",
       "      <td>-1.385543</td>\n",
       "      <td>-2.013074</td>\n",
       "      <td>-0.806089</td>\n",
       "      <td>0.569595</td>\n",
       "      <td>-0.781426</td>\n",
       "      <td>-0.934940</td>\n",
       "      <td>1.325795</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.317779</td>\n",
       "      <td>-0.568226</td>\n",
       "      <td>-1.261251</td>\n",
       "      <td>-0.186631</td>\n",
       "      <td>-0.741824</td>\n",
       "      <td>-0.172534</td>\n",
       "      <td>-0.388082</td>\n",
       "      <td>2.019594</td>\n",
       "      <td>0.810631</td>\n",
       "      <td>-0.395404</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>1825 rows  65 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "      principal component 1  principal component 2  principal component 3  \\\n",
       "0                 -0.960615               2.637565              -1.411701   \n",
       "1                 -0.420814              -2.666090              -1.880717   \n",
       "2                  2.163969               0.082146               1.819162   \n",
       "3                 -0.750427              -2.950321              -1.883113   \n",
       "4                 -0.429275              -0.067060               3.014580   \n",
       "...                     ...                    ...                    ...   \n",
       "1820              -0.382963               0.120614               0.992047   \n",
       "1821              -0.477665               2.834703              -1.989160   \n",
       "1822              -0.016520               0.108308               1.895882   \n",
       "1823              -0.753309              -3.013575              -1.932386   \n",
       "1824              -0.713694              -0.068698               2.177190   \n",
       "\n",
       "      principal component 4  principal component 5  principal component 6  \\\n",
       "0                 -0.879872              -1.351151              -1.341275   \n",
       "1                 -0.502217              -0.399477              -0.516453   \n",
       "2                 -1.228000              -0.341767               1.427034   \n",
       "3                 -0.501615               0.724095              -0.888319   \n",
       "4                 -2.265537               0.520638              -0.044892   \n",
       "...                     ...                    ...                    ...   \n",
       "1820               2.555985               0.911797              -1.354328   \n",
       "1821              -0.635043               1.164131              -0.308848   \n",
       "1822              -1.411357               0.656693              -1.277562   \n",
       "1823              -0.453725               1.176477               1.475854   \n",
       "1824              -1.385543              -2.013074              -0.806089   \n",
       "\n",
       "      principal component 7  principal component 8  principal component 9  \\\n",
       "0                  1.481301               1.936232              -0.093120   \n",
       "1                  0.209275               0.585650              -2.397758   \n",
       "2                  1.740093               0.836960               0.767482   \n",
       "3                  1.131758              -0.430359               2.837142   \n",
       "4                  1.433443              -1.153129               1.642724   \n",
       "...                     ...                    ...                    ...   \n",
       "1820               0.605447              -1.813473              -2.353026   \n",
       "1821              -1.573901              -0.976429               0.401069   \n",
       "1822              -0.356176              -0.310755              -2.080121   \n",
       "1823               0.572913               0.858665               0.104913   \n",
       "1824               0.569595              -0.781426              -0.934940   \n",
       "\n",
       "      principal component 10  ...  principal component 56  \\\n",
       "0                   0.807181  ...                0.289426   \n",
       "1                  -0.449515  ...                0.228169   \n",
       "2                  -0.895097  ...                0.391972   \n",
       "3                   0.211283  ...                0.545031   \n",
       "4                   0.464771  ...               -0.130639   \n",
       "...                      ...  ...                     ...   \n",
       "1820               -0.925162  ...                1.378532   \n",
       "1821               -0.372293  ...                0.323943   \n",
       "1822               -1.743331  ...                0.838994   \n",
       "1823                0.974299  ...                2.106992   \n",
       "1824                1.325795  ...               -0.317779   \n",
       "\n",
       "      principal component 57  principal component 58  principal component 59  \\\n",
       "0                  -0.089653               -0.898951                0.284482   \n",
       "1                  -0.791927                1.346568               -0.133753   \n",
       "2                   0.332891                0.168479                0.257417   \n",
       "3                   0.154213               -1.004270                0.474016   \n",
       "4                   1.206365               -2.057215               -0.944295   \n",
       "...                      ...                     ...                     ...   \n",
       "1820                0.710236               -0.805937               -1.463426   \n",
       "1821               -1.273216               -0.852462                1.278595   \n",
       "1822                0.966531               -0.805958               -0.234253   \n",
       "1823                0.384889                1.724314                2.206176   \n",
       "1824               -0.568226               -1.261251               -0.186631   \n",
       "\n",
       "      principal component 60  principal component 61  principal component 62  \\\n",
       "0                   0.589233                1.837274               -0.503363   \n",
       "1                   0.328647               -0.397283                1.479893   \n",
       "2                  -0.250484                1.624782               -0.233683   \n",
       "3                   0.080496               -0.605871                0.669171   \n",
       "4                   3.539220               -0.574644               -0.472738   \n",
       "...                      ...                     ...                     ...   \n",
       "1820               -1.450543               -1.115401                0.650508   \n",
       "1821               -1.656841               -2.258364               -1.160130   \n",
       "1822               -1.103178                0.253142                0.691452   \n",
       "1823               -0.079748               -0.889737                0.828298   \n",
       "1824               -0.741824               -0.172534               -0.388082   \n",
       "\n",
       "      principal component 63  principal component 64  principal component 65  \n",
       "0                   0.124744               -0.916952               -2.068852  \n",
       "1                  -0.508633                1.233658                0.835310  \n",
       "2                   0.158186               -1.639428               -0.194715  \n",
       "3                   0.733169                0.058910               -0.379444  \n",
       "4                  -0.983877               -0.294182               -0.533892  \n",
       "...                      ...                     ...                     ...  \n",
       "1820               -1.123987               -1.116167                1.409609  \n",
       "1821               -0.192061               -0.821369                0.449265  \n",
       "1822               -1.389206               -0.518220                1.557533  \n",
       "1823                0.262213               -0.794399               -0.469089  \n",
       "1824                2.019594                0.810631               -0.395404  \n",
       "\n",
       "[1825 rows x 65 columns]"
      ]
     },
     "execution_count": 283,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "principalDf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 284,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([0.06635939, 0.04520431, 0.0395337 , 0.03157668, 0.01922997,\n",
       "       0.01872012, 0.01527342, 0.01515496, 0.01490401, 0.01477337,\n",
       "       0.01470153, 0.01449508, 0.01441565, 0.01419555, 0.01350333,\n",
       "       0.01338289, 0.01309098, 0.01260149, 0.01250791, 0.01237308,\n",
       "       0.01233262, 0.01232588, 0.01231805, 0.01230947, 0.01230341,\n",
       "       0.01229718, 0.0122953 , 0.01229084, 0.01228875, 0.01228781,\n",
       "       0.01228459, 0.01228249, 0.01228112, 0.01227998, 0.01227718,\n",
       "       0.01227426, 0.01227379, 0.01227144, 0.01227066, 0.012269  ,\n",
       "       0.01226805, 0.01226425, 0.01226301, 0.01225942, 0.01225598,\n",
       "       0.01225399, 0.01225204, 0.01224959, 0.01224662, 0.01223712,\n",
       "       0.01223317, 0.01222753, 0.0122104 , 0.01207063, 0.01198121,\n",
       "       0.01186728, 0.01172484, 0.01138959, 0.01115314, 0.01106656,\n",
       "       0.01102811, 0.01087309, 0.01066424, 0.01045807, 0.01028075])"
      ]
     },
     "execution_count": 284,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pca.explained_variance_ratio_\n",
    "#95% variance explained"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 292,
   "metadata": {},
   "outputs": [],
   "source": [
    "X = principalDf\n",
    "y = df[\"AveragePrice\"]\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, random_state = 0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 293,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "K Neighbour after PCA\n",
      "Train score: 0.48113\n",
      "Test score: 0.06923\n"
     ]
    }
   ],
   "source": [
    "##### from sklearn.model_selection import cross_val_score\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "\n",
    "knn = KNeighborsRegressor(5) \n",
    "knn.fit(X_train, y_train)\n",
    "\n",
    "print('K Neighbour after PCA')\n",
    "print('Train score: {:.5f}'.format(knn.score(X_train, y_train)))\n",
    "print('Test score: {:.5f}'.format(knn.score(X_test, y_test)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 294,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "K Neighbour after PCA\n",
      "Train score: 0.48113\n",
      "Test score: 0.06923\n",
      "\n",
      "Linear Regression after PCA\n",
      "Train score: 0.5557\n",
      "Test score: 0.4252\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\users\\sneha liza george\\appdata\\local\\programs\\python\\python37\\lib\\site-packages\\sklearn\\svm\\_base.py:947: ConvergenceWarning: Liblinear failed to converge, increase the number of iterations.\n",
      "  \"the number of iterations.\", ConvergenceWarning)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Linear SVC after PCA\n",
      "Train score: 0.5003\n",
      "Test score: 0.4514\n",
      "\n",
      "Kernel Linear SVR after PCA\n",
      "Train score: 0.5410\n",
      "Test score: 0.4277\n",
      "\n",
      "Kernel rbf SVR after PCA\n",
      "Train score: 0.9181\n",
      "Test score: 0.1815\n",
      "\n",
      "Kernel poly SVR after PCA\n",
      "Train score: 0.9397\n",
      "Test score: 0.2795\n"
     ]
    }
   ],
   "source": [
    "from sklearn.svm import SVR\n",
    "from sklearn.svm import LinearSVR\n",
    "\n",
    "knn = KNeighborsRegressor(5) \n",
    "knn.fit(X_train, y_train)\n",
    "\n",
    "print('K Neighbour after PCA')\n",
    "print('Train score: {:.5f}'.format(knn.score(X_train, y_train)))\n",
    "print('Test score: {:.5f}'.format(knn.score(X_test, y_test)))\n",
    "\n",
    "lreg = LinearRegression()\n",
    "lreg.fit(X_train, y_train) \n",
    "\n",
    "print('\\nLinear Regression after PCA')\n",
    "print('Train score: {:.4f}'.format(lreg.score(X_train, y_train)))\n",
    "print('Test score: {:.4f}'.format(lreg.score(X_test, y_test)))\n",
    "\n",
    "clflin = LinearSVR()\n",
    "clflin.fit(X,y)\n",
    "print('\\nLinear SVC after PCA')\n",
    "print('Train score: {:.4f}'.format(clflin.score(X_train, y_train)))\n",
    "print('Test score: {:.4f}'.format(clflin.score(X_test, y_test)))\n",
    "\n",
    "clf2 = SVR(kernel='linear',C=1)\n",
    "clf2.fit(X_train, y_train)\n",
    "\n",
    "print('\\nKernel Linear SVR after PCA')\n",
    "print('Train score: {:.4f}'.format(clf2.score(X_train, y_train)))\n",
    "print('Test score: {:.4f}'.format(clf2.score(X_test, y_test)))\n",
    "\n",
    "clf3 = SVR(kernel='rbf',C=1,gamma=.1)\n",
    "clf3.fit(X_train, y_train)\n",
    "\n",
    "print('\\nKernel rbf SVR after PCA')\n",
    "print('Train score: {:.4f}'.format(clf3.score(X_train, y_train)))\n",
    "print('Test score: {:.4f}'.format(clf3.score(X_test, y_test)))\n",
    "\n",
    "clf4 = SVR(kernel='poly',C=1,gamma=.1)\n",
    "clf4.fit(X_train, y_train)\n",
    "\n",
    "\n",
    "print('\\nKernel poly SVR after PCA')\n",
    "print('Train score: {:.4f}'.format(clf4.score(X_train, y_train)))\n",
    "print('Test score: {:.4f}'.format(clf4.score(X_test, y_test)))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#to be updated\n",
    "Model                   Train  Test     Train after PCA Test after PCA\n",
    "K Neighbour           0.7413    0.5806      0.4811      0.0692\n",
    "Linear regression     0.6469    0.5637      0.5557      0.4252\n",
    "Linear SVR            0.6257    0.5587      0.5186      0.4580\n",
    "Kernel linear SVR     0.6348    0.5644      0.5410      0.4277\n",
    "RBF SVR               0.8019    0.6673      0.9181      0.1815\n",
    "Poly SVR              0.7339    0.5825      0.9397      0.2795"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Deep Learning models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 299,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "y = df['AveragePrice']\n",
    "X = df.drop(['AveragePrice'], axis = 1)\n",
    "X_train, X_test, y_train, y_test = train_test_split(X,y , random_state = 0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 300,
   "metadata": {},
   "outputs": [],
   "source": [
    "# create model\n",
    "from keras.models import Sequential\n",
    "from keras.layers import Dense\n",
    "import numpy\n",
    "\n",
    "model = Sequential()\n",
    "model.add(Dense(13, input_dim=83, kernel_initializer='normal', activation='tanh'))\n",
    "model.add(Dense(1, kernel_initializer='normal'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 301,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Compile model\n",
    "model.compile(loss='mse', optimizer='sgd' , metrics = ['mse'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 302,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/100\n",
      "1368/1368 [==============================] - 0s 144us/step - loss: 0.2459 - mse: 0.2459\n",
      "Epoch 2/100\n",
      "1368/1368 [==============================] - 0s 81us/step - loss: 0.1624 - mse: 0.1624\n",
      "Epoch 3/100\n",
      "1368/1368 [==============================] - 0s 79us/step - loss: 0.1638 - mse: 0.1638\n",
      "Epoch 4/100\n",
      "1368/1368 [==============================] - 0s 93us/step - loss: 0.1666 - mse: 0.1666\n",
      "Epoch 5/100\n",
      "1368/1368 [==============================] - 0s 94us/step - loss: 0.1648 - mse: 0.1648\n",
      "Epoch 6/100\n",
      "1368/1368 [==============================] - 0s 96us/step - loss: 0.1647 - mse: 0.1647\n",
      "Epoch 7/100\n",
      "1368/1368 [==============================] - 0s 92us/step - loss: 0.1649 - mse: 0.1649\n",
      "Epoch 8/100\n",
      "1368/1368 [==============================] - 0s 93us/step - loss: 0.1648 - mse: 0.1648\n",
      "Epoch 9/100\n",
      "1368/1368 [==============================] - 0s 109us/step - loss: 0.1658 - mse: 0.1658\n",
      "Epoch 10/100\n",
      "1368/1368 [==============================] - 0s 93us/step - loss: 0.1655 - mse: 0.1655\n",
      "Epoch 11/100\n",
      "1368/1368 [==============================] - 0s 93us/step - loss: 0.1663 - mse: 0.1663\n",
      "Epoch 12/100\n",
      "1368/1368 [==============================] - 0s 97us/step - loss: 0.1638 - mse: 0.1638\n",
      "Epoch 13/100\n",
      "1368/1368 [==============================] - 0s 95us/step - loss: 0.1650 - mse: 0.1650\n",
      "Epoch 14/100\n",
      "1368/1368 [==============================] - 0s 72us/step - loss: 0.1649 - mse: 0.1649\n",
      "Epoch 15/100\n",
      "1368/1368 [==============================] - 0s 91us/step - loss: 0.1649 - mse: 0.1649\n",
      "Epoch 16/100\n",
      "1368/1368 [==============================] - 0s 91us/step - loss: 0.1644 - mse: 0.1644\n",
      "Epoch 17/100\n",
      "1368/1368 [==============================] - 0s 85us/step - loss: 0.1654 - mse: 0.1654\n",
      "Epoch 18/100\n",
      "1368/1368 [==============================] - 0s 85us/step - loss: 0.1652 - mse: 0.1652\n",
      "Epoch 19/100\n",
      "1368/1368 [==============================] - 0s 100us/step - loss: 0.1642 - mse: 0.1642\n",
      "Epoch 20/100\n",
      "1368/1368 [==============================] - 0s 91us/step - loss: 0.1649 - mse: 0.1649\n",
      "Epoch 21/100\n",
      "1368/1368 [==============================] - 0s 117us/step - loss: 0.1653 - mse: 0.1653\n",
      "Epoch 22/100\n",
      "1368/1368 [==============================] - 0s 80us/step - loss: 0.1654 - mse: 0.1654\n",
      "Epoch 23/100\n",
      "1368/1368 [==============================] - 0s 72us/step - loss: 0.1653 - mse: 0.1653\n",
      "Epoch 24/100\n",
      "1368/1368 [==============================] - 0s 84us/step - loss: 0.1655 - mse: 0.1655\n",
      "Epoch 25/100\n",
      "1368/1368 [==============================] - 0s 86us/step - loss: 0.1654 - mse: 0.1654\n",
      "Epoch 26/100\n",
      "1368/1368 [==============================] - 0s 101us/step - loss: 0.1646 - mse: 0.1646\n",
      "Epoch 27/100\n",
      "1368/1368 [==============================] - 0s 97us/step - loss: 0.1660 - mse: 0.1660\n",
      "Epoch 28/100\n",
      "1368/1368 [==============================] - 0s 89us/step - loss: 0.1657 - mse: 0.1657\n",
      "Epoch 29/100\n",
      "1368/1368 [==============================] - 0s 102us/step - loss: 0.1646 - mse: 0.1646\n",
      "Epoch 30/100\n",
      "1368/1368 [==============================] - 0s 92us/step - loss: 0.1650 - mse: 0.1650\n",
      "Epoch 31/100\n",
      "1368/1368 [==============================] - 0s 114us/step - loss: 0.1651 - mse: 0.1651\n",
      "Epoch 32/100\n",
      "1368/1368 [==============================] - 0s 84us/step - loss: 0.1655 - mse: 0.1655\n",
      "Epoch 33/100\n",
      "1368/1368 [==============================] - 0s 103us/step - loss: 0.1650 - mse: 0.1650 0s - loss: 0.1624 - mse: 0.16\n",
      "Epoch 34/100\n",
      "1368/1368 [==============================] - 0s 89us/step - loss: 0.1654 - mse: 0.1654\n",
      "Epoch 35/100\n",
      "1368/1368 [==============================] - 0s 111us/step - loss: 0.1652 - mse: 0.1652\n",
      "Epoch 36/100\n",
      "1368/1368 [==============================] - 0s 103us/step - loss: 0.1647 - mse: 0.1647\n",
      "Epoch 37/100\n",
      "1368/1368 [==============================] - 0s 96us/step - loss: 0.1660 - mse: 0.1660\n",
      "Epoch 38/100\n",
      "1368/1368 [==============================] - 0s 93us/step - loss: 0.1655 - mse: 0.1655\n",
      "Epoch 39/100\n",
      "1368/1368 [==============================] - 0s 98us/step - loss: 0.1653 - mse: 0.1653\n",
      "Epoch 40/100\n",
      "1368/1368 [==============================] - 0s 92us/step - loss: 0.1654 - mse: 0.1654\n",
      "Epoch 41/100\n",
      "1368/1368 [==============================] - 0s 93us/step - loss: 0.1654 - mse: 0.1654\n",
      "Epoch 42/100\n",
      "1368/1368 [==============================] - 0s 91us/step - loss: 0.1660 - mse: 0.1660\n",
      "Epoch 43/100\n",
      "1368/1368 [==============================] - 0s 98us/step - loss: 0.1652 - mse: 0.1652\n",
      "Epoch 44/100\n",
      "1368/1368 [==============================] - 0s 87us/step - loss: 0.1646 - mse: 0.1646\n",
      "Epoch 45/100\n",
      "1368/1368 [==============================] - 0s 93us/step - loss: 0.1652 - mse: 0.1652\n",
      "Epoch 46/100\n",
      "1368/1368 [==============================] - 0s 104us/step - loss: 0.1648 - mse: 0.1648\n",
      "Epoch 47/100\n",
      "1368/1368 [==============================] - 0s 84us/step - loss: 0.1643 - mse: 0.1643\n",
      "Epoch 48/100\n",
      "1368/1368 [==============================] - 0s 85us/step - loss: 0.1659 - mse: 0.1659\n",
      "Epoch 49/100\n",
      "1368/1368 [==============================] - 0s 90us/step - loss: 0.1653 - mse: 0.1653\n",
      "Epoch 50/100\n",
      "1368/1368 [==============================] - 0s 88us/step - loss: 0.1647 - mse: 0.1647\n",
      "Epoch 51/100\n",
      "1368/1368 [==============================] - 0s 89us/step - loss: 0.1650 - mse: 0.1650\n",
      "Epoch 52/100\n",
      "1368/1368 [==============================] - 0s 96us/step - loss: 0.1661 - mse: 0.1661\n",
      "Epoch 53/100\n",
      "1368/1368 [==============================] - 0s 85us/step - loss: 0.1644 - mse: 0.1644\n",
      "Epoch 54/100\n",
      "1368/1368 [==============================] - 0s 89us/step - loss: 0.1659 - mse: 0.1659\n",
      "Epoch 55/100\n",
      "1368/1368 [==============================] - 0s 90us/step - loss: 0.1654 - mse: 0.1654\n",
      "Epoch 56/100\n",
      "1368/1368 [==============================] - 0s 101us/step - loss: 0.1655 - mse: 0.1655\n",
      "Epoch 57/100\n",
      "1368/1368 [==============================] - 0s 92us/step - loss: 0.1643 - mse: 0.1643\n",
      "Epoch 58/100\n",
      "1368/1368 [==============================] - 0s 88us/step - loss: 0.1655 - mse: 0.1655\n",
      "Epoch 59/100\n",
      "1368/1368 [==============================] - 0s 99us/step - loss: 0.1652 - mse: 0.1652\n",
      "Epoch 60/100\n",
      "1368/1368 [==============================] - 0s 74us/step - loss: 0.1656 - mse: 0.1656\n",
      "Epoch 61/100\n",
      "1368/1368 [==============================] - 0s 61us/step - loss: 0.1648 - mse: 0.1648\n",
      "Epoch 62/100\n",
      "1368/1368 [==============================] - 0s 98us/step - loss: 0.1653 - mse: 0.1653\n",
      "Epoch 63/100\n",
      "1368/1368 [==============================] - 0s 86us/step - loss: 0.1652 - mse: 0.1652\n",
      "Epoch 64/100\n",
      "1368/1368 [==============================] - 0s 86us/step - loss: 0.1649 - mse: 0.1649\n",
      "Epoch 65/100\n",
      "1368/1368 [==============================] - 0s 84us/step - loss: 0.1647 - mse: 0.1647\n",
      "Epoch 66/100\n",
      "1368/1368 [==============================] - 0s 90us/step - loss: 0.1657 - mse: 0.1657\n",
      "Epoch 67/100\n",
      "1368/1368 [==============================] - 0s 85us/step - loss: 0.1657 - mse: 0.1657\n",
      "Epoch 68/100\n",
      "1368/1368 [==============================] - 0s 108us/step - loss: 0.1658 - mse: 0.1658\n",
      "Epoch 69/100\n",
      "1368/1368 [==============================] - 0s 120us/step - loss: 0.1654 - mse: 0.1654\n",
      "Epoch 70/100\n",
      "1368/1368 [==============================] - 0s 120us/step - loss: 0.1647 - mse: 0.1647\n",
      "Epoch 71/100\n",
      "1368/1368 [==============================] - 0s 120us/step - loss: 0.1654 - mse: 0.1654\n",
      "Epoch 72/100\n",
      "1368/1368 [==============================] - 0s 105us/step - loss: 0.1641 - mse: 0.1641\n",
      "Epoch 73/100\n",
      "1368/1368 [==============================] - 0s 114us/step - loss: 0.1661 - mse: 0.1661\n",
      "Epoch 74/100\n",
      "1368/1368 [==============================] - 0s 124us/step - loss: 0.1655 - mse: 0.1655\n",
      "Epoch 75/100\n",
      "1368/1368 [==============================] - 0s 85us/step - loss: 0.1652 - mse: 0.1652\n",
      "Epoch 76/100\n",
      "1368/1368 [==============================] - 0s 96us/step - loss: 0.1652 - mse: 0.1652\n",
      "Epoch 77/100\n",
      "1368/1368 [==============================] - 0s 97us/step - loss: 0.1651 - mse: 0.1651\n",
      "Epoch 78/100\n",
      "1368/1368 [==============================] - 0s 93us/step - loss: 0.1653 - mse: 0.1653\n",
      "Epoch 79/100\n",
      "1368/1368 [==============================] - 0s 95us/step - loss: 0.1649 - mse: 0.1649\n",
      "Epoch 80/100\n",
      "1368/1368 [==============================] - 0s 96us/step - loss: 0.1654 - mse: 0.1654\n",
      "Epoch 81/100\n",
      "1368/1368 [==============================] - 0s 87us/step - loss: 0.1657 - mse: 0.1657\n",
      "Epoch 82/100\n",
      "1368/1368 [==============================] - 0s 79us/step - loss: 0.1648 - mse: 0.1648\n",
      "Epoch 83/100\n",
      "1368/1368 [==============================] - 0s 85us/step - loss: 0.1657 - mse: 0.1657\n",
      "Epoch 84/100\n",
      "1368/1368 [==============================] - 0s 74us/step - loss: 0.1652 - mse: 0.1652\n",
      "Epoch 85/100\n",
      "1368/1368 [==============================] - 0s 69us/step - loss: 0.1661 - mse: 0.1661\n",
      "Epoch 86/100\n",
      "1368/1368 [==============================] - 0s 69us/step - loss: 0.1658 - mse: 0.1658\n",
      "Epoch 87/100\n",
      "1368/1368 [==============================] - 0s 74us/step - loss: 0.1638 - mse: 0.1638\n",
      "Epoch 88/100\n",
      "1368/1368 [==============================] - 0s 77us/step - loss: 0.1663 - mse: 0.1663\n",
      "Epoch 89/100\n",
      "1368/1368 [==============================] - 0s 74us/step - loss: 0.1652 - mse: 0.1652\n",
      "Epoch 90/100\n",
      "1368/1368 [==============================] - 0s 70us/step - loss: 0.1657 - mse: 0.1657\n",
      "Epoch 91/100\n",
      "1368/1368 [==============================] - 0s 77us/step - loss: 0.1649 - mse: 0.1649\n",
      "Epoch 92/100\n",
      "1368/1368 [==============================] - 0s 69us/step - loss: 0.1656 - mse: 0.1656\n",
      "Epoch 93/100\n",
      "1368/1368 [==============================] - 0s 72us/step - loss: 0.1646 - mse: 0.1646\n",
      "Epoch 94/100\n",
      "1368/1368 [==============================] - 0s 79us/step - loss: 0.1654 - mse: 0.1654\n",
      "Epoch 95/100\n",
      "1368/1368 [==============================] - 0s 72us/step - loss: 0.1643 - mse: 0.1643\n",
      "Epoch 96/100\n",
      "1368/1368 [==============================] - 0s 70us/step - loss: 0.1655 - mse: 0.1655\n",
      "Epoch 97/100\n",
      "1368/1368 [==============================] - 0s 69us/step - loss: 0.1652 - mse: 0.1652\n",
      "Epoch 98/100\n",
      "1368/1368 [==============================] - 0s 70us/step - loss: 0.1656 - mse: 0.1656\n",
      "Epoch 99/100\n",
      "1368/1368 [==============================] - 0s 70us/step - loss: 0.1654 - mse: 0.1654\n",
      "Epoch 100/100\n",
      "1368/1368 [==============================] - 0s 74us/step - loss: 0.1655 - mse: 0.1655\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.callbacks.callbacks.History at 0x13d25795808>"
      ]
     },
     "execution_count": 302,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.fit(X_train, y_train, epochs = 100, batch_size = 20)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 303,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "457/457 [==============================] - 0s 116us/step\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[0.15344429198709567, 0.1534442901611328]"
      ]
     },
     "execution_count": 303,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.evaluate(X_test, y_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 304,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train score: -0.01\n",
      "Test score: -0.02\n"
     ]
    }
   ],
   "source": [
    "from sklearn.metrics import r2_score, recall_score, precision_score\n",
    "\n",
    "y_train_predict = model.predict(X_train)\n",
    "y_test_predict = model.predict(X_test)\n",
    "\n",
    "print('Train score: {:.2f}'.format(r2_score(y_train, y_train_predict)))\n",
    "print('Test score: {:.2f}'.format(r2_score(y_test, y_test_predict)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Grid Search"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 320,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from sklearn.model_selection import GridSearchCV\n",
    "from keras.models import Sequential\n",
    "from keras.layers import Dense\n",
    "from keras.wrappers.scikit_learn import KerasClassifier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 321,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "y = df['AveragePrice']\n",
    "X = df.drop(['AveragePrice'], axis = 1)\n",
    "X_train, X_test, y_train, y_test = train_test_split(X,y , random_state = 0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 322,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_model():\n",
    "    #create model\n",
    "    model = Sequential()\n",
    "    model.add(Dense(12, input_dim=83, activation='relu'))\n",
    "    model.add(Dense(8, activation='relu'))\n",
    "    model.add(Dense(1, activation='sigmoid'))\n",
    "    #compile model\n",
    "    model.compile(loss='binary_crossentropy', optimizer='adam', metrics=['accuracy'])\n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 323,
   "metadata": {},
   "outputs": [],
   "source": [
    "np.random.seed(0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 324,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = KerasClassifier(build_fn = create_model, verbose = 0)\n",
    "\n",
    "param_grid = {'batch_size':[10,20,30,40] , 'epochs':[10, 50, 100]}\n",
    "grid_search = GridSearchCV(estimator= model, param_grid = param_grid, cv = 5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 325,
   "metadata": {},
   "outputs": [],
   "source": [
    "grid_search_result = grid_search.fit(X, y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 326,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "GridSearchCV(cv=5, error_score=nan,\n",
       "             estimator=<keras.wrappers.scikit_learn.KerasClassifier object at 0x0000013D3E557FC8>,\n",
       "             iid='deprecated', n_jobs=None,\n",
       "             param_grid={'batch_size': [10, 20, 30, 40],\n",
       "                         'epochs': [10, 50, 100]},\n",
       "             pre_dispatch='2*n_jobs', refit=True, return_train_score=False,\n",
       "             scoring=None, verbose=0)"
      ]
     },
     "execution_count": 326,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "grid_search_result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 319,
   "metadata": {},
   "outputs": [
    {
     "ename": "AttributeError",
     "evalue": "'GridSearchCV' object has no attribute 'best_params_'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-319-06d255202806>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[1;32m----> 1\u001b[1;33m \u001b[0mprint\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m\"Best parameters: {}\"\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mformat\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mgrid_search\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mbest_params_\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[1;31mAttributeError\u001b[0m: 'GridSearchCV' object has no attribute 'best_params_'"
     ]
    }
   ],
   "source": [
    "print(\"Best parameters: {}\".format(grid_search.best_params_))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# create model\n",
    "model = Sequential()\n",
    "model.add(Dense(12, input_dim=83, activation='relu'))\n",
    "model.add(Dense(8, activation='relu'))\n",
    "model.add(Dense(1, activation='sigmoid'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Compile model\n",
    "model.compile(loss='binary_crossentropy', optimizer='adam', metrics=['accuracy'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Fit the model\n",
    "model.fit(X_train, y_train, epochs=100, batch_size=20)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# evaluate the model\n",
    "scores = model.evaluate(X_test, y_test)\n",
    "print(\"\\n%s: %.2f%%\" % (model.metrics_names[1], scores[1]*100))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.evaluate(X_test, y_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print('Train score: {:.2f}'.format(r2_score(y_train, y_train_predict)))\n",
    "print('Test score: {:.2f}'.format(r2_score(y_test, y_test_predict)))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
