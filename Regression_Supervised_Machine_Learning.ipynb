{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# <font color='Black'> Project 1 Regression"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#importing libraries\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#reading data\n",
    "df=pd.read_csv('avocado2.csv')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df=df.sample(frac=.1,random_state=0)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Data Exploration and Preprocessing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.describe()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#finding correlations between numerical features\n",
    "import seaborn as sns\n",
    "df1=df.iloc[:,3:9]\n",
    "f, ax = plt.subplots(figsize=(10, 8))\n",
    "corr = df1.corr()\n",
    "sns.heatmap(corr, mask=np.zeros_like(corr, dtype=np.bool), cmap=sns.diverging_palette(220, 10, as_cmap=True),\n",
    "            square=True, ax=ax)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#displaying columns having null values\n",
    "df.columns[df.isnull().any()]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#imputing NAN values with mean\n",
    "df['AveragePrice'] = df.groupby('Type')['AveragePrice'].transform(lambda x: x.fillna(x.mean()))\n",
    "df['XLarge Bags'] = df.groupby('Type')['XLarge Bags'].transform(lambda x: x.fillna(x.mean()))\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Mapping binary variables to 0 and 1 \n",
    "df.Rain = df.Rain.replace({'No' : 0, 'Yes' : 1}) \n",
    "df.Snow = df.Snow.replace({'No' : 0, 'Yes' : 1})\n",
    "df.Type = df.Type.replace({'conventional' : 0, 'organic' : 1}) \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Replacing values with one hot vector\n",
    "df = pd.get_dummies(df, columns = ['Year'], prefix = ['Year'])\n",
    "df = pd.get_dummies(df, columns = ['Region'], prefix = ['Region'])\n",
    "df = pd.get_dummies(df, columns = ['Month'], prefix = ['Month'])\n",
    "df = pd.get_dummies(df, columns = ['Season'], prefix = ['Season'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Dropping unnecessary/ redundant columns\n",
    "df.drop(['Date'],axis=1,inplace= True)\n",
    "df.drop(['Sl.No'],axis=1,inplace= True)\n",
    "df.drop(['Total Volume'],axis=1,inplace= True)\n",
    "df.drop(['Total Bags'],axis=1,inplace= True)\n",
    "df.drop(['Month Number'],axis=1,inplace= True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Creating Feature and target variable\n",
    "y = df['AveragePrice']\n",
    "X = df.drop(['AveragePrice'], axis = 1)\n",
    "names = list(X.columns.values)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Scaling & splitting into train and test dataset\n",
    "from sklearn.neighbors import KNeighborsRegressor\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.preprocessing import MinMaxScaler\n",
    "from sklearn.model_selection import train_test_split\n",
    "X_train_org, X_test_org, y_train, y_test = train_test_split(X,y, random_state = 0)\n",
    "scaler = MinMaxScaler()\n",
    "X_train = scaler.fit_transform(X_train_org)\n",
    "X_test = scaler.transform(X_test_org)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "StandardScaler removes the mean and scales the data to unit variance. However, the outliers have an influence when \n",
    "computing the empirical mean and standard deviation which shrink the range of the feature values Standard Scaler therefore \n",
    "cannot guarantee balanced feature scales in the presence of outliers.\n",
    "MinMaxScaler rescales the data set such that all feature values are in the range [0, 1] As StandardScaler, MinMaxScaler is\n",
    "very sensitive to the presence of outliers.\n",
    "It doesn't really matter which scaling is used in this particular dataset"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Grid Search"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.svm import SVR\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "X_train_org, X_test_org, y_train, y_test = train_test_split(X,y, random_state = 0)\n",
    "scaler = MinMaxScaler()\n",
    "X_train = scaler.fit_transform(X_train_org)\n",
    "X_test = scaler.transform(X_test_org)\n",
    "print(\"Size of training set: {}   size of test set: {}\".format(X_train.shape[0], X_test.shape[0]))\n",
    "\n",
    "best_score = 0\n",
    "\n",
    "for gamma in [0.001, 0.01, 0.1, 1, 10, 100]:\n",
    "    for C in [0.001, 0.01, 0.1, 1, 10, 100]:\n",
    "        # for each combination of parameters, train an SVR\n",
    "        svm = SVR(gamma=gamma, C=C)\n",
    "        svm.fit(X_train, y_train)\n",
    "        # evaluate the SVC on the test set\n",
    "        score = svm.score(X_test, y_test)\n",
    "        # if we got a better score, store the score and parameters\n",
    "        if score > best_score:\n",
    "            best_score = score\n",
    "            best_parameters = {'C': C, 'gamma': gamma}\n",
    "\n",
    "print(\"Best score: {:.2f}\".format(best_score))\n",
    "print(\"Best parameters: {}\".format(best_parameters))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.svm import SVR\n",
    "\n",
    "y = df['AveragePrice']\n",
    "X = df.drop(['AveragePrice'], axis = 1)\n",
    "X_train_org, X_test_org, y_train, y_test = train_test_split(X,y, random_state = 0)\n",
    "scaler = MinMaxScaler()\n",
    "X_train = scaler.fit_transform(X_train_org)\n",
    "X_test = scaler.transform(X_test_org)\n",
    "\n",
    "# split data into train+validation set and test set\n",
    "X_trainval, X_test, y_trainval, y_test = train_test_split(X_train,y_train, random_state = 0)\n",
    "\n",
    "# split train+validation set into training and validation sets\n",
    "X_train, X_valid, y_train, y_valid = train_test_split(X_trainval, y_trainval, random_state=1)\n",
    "\n",
    "\n",
    "print(\"Size of training set: {}   size of validation set: {}   size of test set:\"\n",
    "      \" {}\\n\".format(X_train.shape[0], X_valid.shape[0], X_test.shape[0]))\n",
    "\n",
    "best_score = 0\n",
    "\n",
    "for gamma in [0.001, 0.01, 0.1, 1, 10, 100]:\n",
    "    for C in [0.001, 0.01, 0.1, 1, 10, 100]:\n",
    "        # for each combination of parameters, train an SVC\n",
    "        svm = SVR(gamma=gamma, C=C)\n",
    "        svm.fit(X_train, y_train)\n",
    "        # evaluate the SVC on the validation set\n",
    "        score = svm.score(X_valid, y_valid)\n",
    "        # if we got a better score, store the score and parameters\n",
    "        if score > best_score:\n",
    "            best_score = score\n",
    "            best_parameters = {'C': C, 'gamma': gamma}\n",
    "\n",
    "# rebuild a model on the combined training and validation set,\n",
    "# and evaluate it on the test set\n",
    "svm = SVR(**best_parameters)\n",
    "svm.fit(X_trainval, y_trainval)\n",
    "test_score = svm.score(X_test, y_test)\n",
    "print(\"Best score on validation set: {:.2f}\".format(best_score))\n",
    "print(\"Best parameters: \", best_parameters)\n",
    "print(\"Test set score with best parameters: {:.2f}\".format(test_score))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Grid search with cross validation\n",
    "from sklearn.model_selection import cross_val_score\n",
    "import numpy as np\n",
    "\n",
    "for gamma in [0.001, 0.01, 0.1, 1, 10, 100]:\n",
    "    for C in [0.001, 0.01, 0.1, 1, 10, 100]:\n",
    "        # for each combination of parameters,\n",
    "        # train an SVC\n",
    "        svm = SVR(gamma=gamma, C=C)\n",
    "        # perform cross-validation\n",
    "        scores = cross_val_score(svm, X_trainval, y_trainval, cv=5)\n",
    "        # compute mean cross-validation accuracy\n",
    "        score = np.mean(scores)\n",
    "        # if we got a better score, store the score and parameters\n",
    "        if score > best_score:\n",
    "            best_score = score\n",
    "            best_parameters = {'C': C, 'gamma': gamma}\n",
    "            \n",
    "# rebuild a model on the combined training and validation set\n",
    "svm = SVR(**best_parameters)\n",
    "svm.fit(X_trainval, y_trainval)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "param_grid = {'C': [0.001, 0.01, 0.1, 1, 10, 100],\n",
    "              'gamma': [0.001, 0.01, 0.1, 1, 10, 100]}\n",
    "print(\"Parameter grid:\\n{}\".format(param_grid))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import GridSearchCV\n",
    "from sklearn.svm import SVR\n",
    "grid_search = GridSearchCV(SVR(), param_grid, cv=5, return_train_score=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#X_train, X_test, y_train, y_test = train_test_split(X,y, random_state = 0)\n",
    "y = df['AveragePrice']\n",
    "X = df.drop(['AveragePrice'], axis = 1)\n",
    "X_train_org, X_test_org, y_train, y_test = train_test_split(X,y, random_state = 0)\n",
    "scaler = MinMaxScaler()\n",
    "X_train = scaler.fit_transform(X_train_org)\n",
    "X_test = scaler.transform(X_test_org)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "grid_search.fit(X_train, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"Best parameters: {}\".format(grid_search.best_params_))\n",
    "print(\"Best cross-validation score: {:.2f}\".format(grid_search.best_score_))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "# convert to DataFrame\n",
    "results = pd.DataFrame(grid_search.cv_results_)\n",
    "# show the first 5 rows\n",
    "display(results.head())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%matplotlib notebook\n",
    "import mglearn\n",
    "scores = np.array(results.mean_test_score).reshape(6, 6)\n",
    "\n",
    "# plot the mean cross-validation scores\n",
    "mglearn.tools.heatmap(scores, xlabel='gamma', xticklabels=param_grid['gamma'], ylabel='C', yticklabels=param_grid['C'], cmap=\"viridis\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# knn regressor"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#knn\n",
    "from sklearn.neighbors import KNeighborsRegressor\n",
    "%matplotlib inline\n",
    "train_score_array = []\n",
    "test_score_array = []\n",
    "\n",
    "for k in range(1,20):\n",
    "    knn_reg = KNeighborsRegressor(k)\n",
    "    knn_reg.fit(X_train, y_train)\n",
    "    train_score_array.append(knn_reg.score(X_train, y_train))\n",
    "    test_score_array.append(knn_reg.score(X_test, y_test))\n",
    "\n",
    "x_axis = range(1,20)\n",
    "plt.plot(x_axis, train_score_array, c = 'g', label = 'Train Score')\n",
    "plt.plot(x_axis, test_score_array, c = 'b', label = 'Test Score')\n",
    "plt.legend()\n",
    "plt.xlabel('k')\n",
    "plt.ylabel('MSE')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "knn=KNeighborsRegressor()   \n",
    "knn.fit(X_train,y_train)\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#using cross-validation to find average training score\n",
    "from sklearn.model_selection import cross_val_score\n",
    "scores = cross_val_score(knn, X_train,y_train, cv=5)\n",
    "print(\"Cross-validation scores: {}\".format(scores))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"Average cross-validation score: {:.2f}\".format(scores.mean()))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Linear Regression"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "##Linear Regression\n",
    "from sklearn.linear_model import LinearRegression\n",
    "lreg = LinearRegression()\n",
    "lreg.fit(X_train, y_train)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#using cross-validation to find average training score\n",
    "from sklearn.model_selection import cross_val_score\n",
    "scores = cross_val_score(lreg, X_train,y_train, cv=5)\n",
    "print(\"Cross-validation scores: {}\".format(scores))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"Average cross-validation score: {:.2f}\".format(scores.mean()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#plotting fitted line\n",
    "%matplotlib inline\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "X_train_rm = X_train[:,5].reshape(-1,1)\n",
    "lreg.fit(X_train_rm, y_train)\n",
    "y_predict = lreg.predict(X_train_rm)\n",
    "\n",
    "plt.plot(X_train_rm, y_predict, c = 'r')\n",
    "plt.scatter(X_train_rm,y_train)\n",
    "plt.xlabel('RM')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Ridge regression"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from  sklearn.linear_model import Ridge\n",
    "\n",
    "x_range = [0.01, 0.1, 1, 10, 100]\n",
    "train_score_list = []\n",
    "test_score_list = []\n",
    "\n",
    "for alpha in x_range: \n",
    "    ridge = Ridge(alpha)\n",
    "    ridge.fit(X_train,y_train)\n",
    "    train_score_list.append(ridge.score(X_train,y_train))\n",
    "    test_score_list.append(ridge.score(X_test, y_test))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#plotting train and test scores for different values of alpha\n",
    "%matplotlib inline\n",
    "import matplotlib.pyplot as plt\n",
    "plt.plot(x_range, train_score_list, c = 'g', label = 'Train Score')\n",
    "plt.plot(x_range, test_score_list, c = 'b', label = 'Test Score')\n",
    "plt.xscale('log')\n",
    "plt.legend(loc = 3)\n",
    "plt.xlabel(r'$\\alpha$')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ridge = Ridge(alpha = 1)\n",
    "ridge.fit(X_train,y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#using cross-validation to find average training score\n",
    "from sklearn.model_selection import cross_val_score\n",
    "scores = cross_val_score(ridge, X_train,y_train, cv=5)\n",
    "print(\"Cross-validation scores: {}\".format(scores))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"Average cross-validation score: {:.2f}\".format(scores.mean()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%matplotlib inline\n",
    "import numpy as np\n",
    "\n",
    "x_range1 = np.linspace(0.001, 1, 100).reshape(-1,1)\n",
    "x_range2 = np.linspace(1, 10000, 10000).reshape(-1,1)\n",
    "\n",
    "x_range = np.append(x_range1, x_range2)\n",
    "coeff = []\n",
    "\n",
    "for alpha in x_range: \n",
    "    ridge = Ridge(alpha)\n",
    "    ridge.fit(X_train,y_train)\n",
    "    coeff.append(ridge.coef_ )\n",
    "    \n",
    "coeff = np.array(coeff)\n",
    "\n",
    "for i in range(0,13):\n",
    "    plt.plot(x_range, coeff[:,i], label = 'feature {:d}'.format(i))\n",
    "\n",
    "plt.axhline(y=0, xmin=0.001, xmax=9999, linewidth=1, c ='gray')\n",
    "plt.xlabel(r'$\\alpha$')\n",
    "plt.xscale('log')\n",
    "plt.legend(loc='upper center', bbox_to_anchor=(0.5, 1.5),\n",
    "          ncol=3, fancybox=True, shadow=True)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Lasso Regression"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#running lasso with different values of alpha\n",
    "#It doesn't make sense to do lasso regression for this dataset as the dataset has only around 15 features.\n",
    "from sklearn.linear_model import Lasso\n",
    "x_range = [0.01, 0.1, 1, 10, 100]\n",
    "train_score_list = []\n",
    "test_score_list = []\n",
    "\n",
    "for alpha in x_range: \n",
    "    lasso = Lasso(alpha)\n",
    "    lasso.fit(X_train,y_train)\n",
    "    train_score_list.append(lasso.score(X_train,y_train))\n",
    "    test_score_list.append(lasso.score(X_test, y_test))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#plotting train score and test score for different values of alpha\n",
    "plt.plot(x_range, train_score_list, c = 'g', label = 'Train Score')\n",
    "plt.plot(x_range, test_score_list, c = 'b', label = 'Test Score')\n",
    "plt.xscale('log')\n",
    "plt.legend(loc = 3)\n",
    "plt.xlabel(r'$\\alpha$')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "lasso = Lasso(alpha = .001)\n",
    "lasso.fit(X_train,y_train)\n",
    "print('Train score: {:.4f}'.format(lasso.score(X_train,y_train)))\n",
    "print('Test score: {:.4f}'.format(lasso.score(X_test, y_test)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#using cross-validation to find average training score\n",
    "from sklearn.model_selection import cross_val_score\n",
    "scores = cross_val_score(lasso, X_train,y_train, cv=5)\n",
    "print(\"Cross-validation scores: {}\".format(scores))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"Average cross-validation score on training set: {:.2f}\".format(scores.mean()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%matplotlib inline\n",
    "\n",
    "x_range1 = np.linspace(0.001, 1, 1000).reshape(-1,1)\n",
    "x_range2 = np.linspace(1, 1000, 1000).reshape(-1,1)\n",
    "\n",
    "x_range = np.append(x_range1, x_range2)\n",
    "coeff = []\n",
    "\n",
    "for alpha in x_range: \n",
    "    lasso = Lasso(alpha)\n",
    "    lasso.fit(X_train,y_train)\n",
    "    coeff.append(lasso.coef_ )\n",
    "    \n",
    "coeff = np.array(coeff)\n",
    "\n",
    "for i in range(0,13):\n",
    "    plt.plot(x_range, coeff[:,i], label = 'feature {:d}'.format(i))\n",
    "\n",
    "plt.axhline(y=0, xmin=0.001, xmax=9999, linewidth=1, c ='gray')\n",
    "plt.xlabel(r'$\\alpha$')\n",
    "plt.xscale('log')\n",
    "plt.legend(loc='upper center', bbox_to_anchor=(0.5, 1.5),\n",
    "          ncol=3, fancybox=True, shadow=True)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Polynomial Regression"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from  sklearn.preprocessing  import PolynomialFeatures\n",
    "train_score_list = []\n",
    "test_score_list = []\n",
    "\n",
    "for n in range(1,3):\n",
    "    poly = PolynomialFeatures(n)\n",
    "    X_train_poly = poly.fit_transform(X_train)\n",
    "    X_test_poly = poly.transform(X_test)\n",
    "    lreg.fit(X_train_poly, y_train)\n",
    "    train_score_list.append(lreg.score(X_train_poly, y_train))\n",
    "    test_score_list.append(lreg.score(X_test_poly, y_test))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from  sklearn.preprocessing  import PolynomialFeatures\n",
    "\n",
    "train_score_list = []\n",
    "test_score_list = []\n",
    "\n",
    "for n in range(1,3):\n",
    "    poly = PolynomialFeatures(n)\n",
    "    X_train_poly = poly.fit_transform(X_train)\n",
    "    X_test_poly = poly.transform(X_test)\n",
    "    p=lreg.fit(X_train_poly, y_train)\n",
    "    train_score_list.append(lreg.score(X_train_poly, y_train))\n",
    "    test_score_list.append(lreg.score(X_test_poly, y_test))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(train_score_list)\n",
    "print(test_score_list)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#using cross-validation to find average training score\n",
    "from sklearn.model_selection import cross_val_score\n",
    "scores = cross_val_score(p, X_train,y_train, cv=5)\n",
    "print(\"Cross-validation scores: {}\".format(scores))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"Average cross-validation score: {:.2f}\".format(scores.mean()))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Support Vector Regressor Machine"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.svm import SVR\n",
    "regressor = SVR(kernel='linear',C=1)\n",
    "regressor.fit(X_train,y_train)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#using cross-validation to find average training score\n",
    "from sklearn.model_selection import cross_val_score\n",
    "scores = cross_val_score(regressor, X_train,y_train, cv=5)\n",
    "print(\"Cross-validation scores: {}\".format(scores))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"Average cross-validation score on training set: {:.2f}\".format(scores.mean()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.svm import SVR\n",
    "rreg = SVR(kernel='rbf',C=1,gamma=.1)\n",
    "rreg.fit(X_train,y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#using cross-validation to find average training score\n",
    "from sklearn.model_selection import cross_val_score\n",
    "scores = cross_val_score(rreg, X_train,y_train, cv=5)\n",
    "print(\"Cross-validation scores: {}\".format(scores))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"Average cross-validation score on training set: {:.2f}\".format(scores.mean()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.svm import SVR\n",
    "preg = SVR(kernel='poly',C=1,gamma=.1)\n",
    "preg.fit(X_train,y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#using cross-validation to find average training score\n",
    "from sklearn.model_selection import cross_val_score\n",
    "scores = cross_val_score(preg, X_train,y_train, cv=5)\n",
    "print(\"Cross-validation scores: {}\".format(scores))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"Average cross-validation score on training set: {:.2f}\".format(scores.mean()))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Choosing the best model "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Based on accuarcy, e choose the model that gives the best results\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.preprocessing import MinMaxScaler\n",
    "from sklearn.model_selection import train_test_split\n",
    "X_train_org, X_test_org, y_train, y_test = train_test_split(X,y, random_state = 0)\n",
    "scaler = MinMaxScaler()\n",
    "X_train = scaler.fit_transform(X_train_org)\n",
    "X_test = scaler.transform(X_test_org)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ridge = Ridge(alpha = 1)\n",
    "ridge.fit(X_train,y_train)\n",
    "print(ridge.score(X_train, y_train))\n",
    "print(ridge.score(X_test, y_test))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# <font color='Black'> Project 1 Classification"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Data Preprocessing "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "np.random.seed(0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.read_csv ('food-inspections.csv')\n",
    "df.head()\n",
    "#df = df[:2000]\n",
    "df=df.sample(frac = 0.01016, random_state=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.drop('DBA Name', axis = 1, inplace =True)\n",
    "df.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.info()\n",
    "df.columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.drop('AKA Name', axis = 1, inplace =True)\n",
    "df.drop('Address', axis = 1, inplace =True)\n",
    "df.drop('City', axis = 1, inplace =True)\n",
    "df.drop('State', axis = 1, inplace =True)\n",
    "df.drop('Inspection Date', axis = 1, inplace =True)\n",
    "df.drop('Inspection Type', axis = 1, inplace =True)\n",
    "df.drop('Latitude', axis = 1, inplace =True)\n",
    "df.drop('Longitude', axis = 1, inplace =True)\n",
    "df.drop('Location', axis = 1, inplace =True)\n",
    "df.drop('Historical Wards 2003-2015', axis = 1, inplace =True)\n",
    "df.drop('Zip Codes', axis = 1, inplace =True)\n",
    "df.drop('Community Areas', axis = 1, inplace =True)\n",
    "df.drop('Census Tracts', axis = 1, inplace =True)\n",
    "df.drop('Wards', axis = 1, inplace =True)\n",
    "df.columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df['Facility Type'].unique()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.drop('Facility Type', axis = 1, inplace =True)\n",
    "df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df['Risk'].unique()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = df.dropna(axis=0, subset=['Risk'])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = df.dropna(axis=0, subset=['License #'])\n",
    "df = df.dropna(axis=0, subset=['Zip'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df['Risk'].unique()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_cols = pd.get_dummies(df['Risk'], prefix='Risk')\n",
    "df[df_cols.columns] = df_cols\n",
    "df.drop('Risk', axis = 1, inplace =True)\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df['Violations'].unique()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df['Violations'] = df['Violations'].replace(np.nan,0)\n",
    "df['Violations'] = df['Violations'].replace('-',0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "m = df.Violations != 0\n",
    "df.Violations.where(~m,other='1')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df['Violation_1_0'] = m\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df['Violation_1_0'] = np.round(df['Violation_1_0']).astype(int)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df['Violation_1_0'] = df['Violation_1_0'].replace('True',1)\n",
    "df['Violation_1_0'] = df['Violation_1_0'].replace('False',0)\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.drop('Violations', axis = 1, inplace =True)\n",
    "df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df['Results'].unique()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df['Results'] = (df['Results']).astype(str)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df['Results'] = df['Results'].replace('Not Ready',np.nan)\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df['Results'] = df['Results'].replace('Out of Business',np.nan)\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df['Results'] = df['Results'].replace('No Entry',np.nan)\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df['Results'] = df['Results'].replace('Business Not Located',np.nan)\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = df.dropna(axis=0, subset=['Results'])\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df['Results'].unique()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df['Results'] = df['Results'].replace('Pass w/ Conditions',1)\n",
    "df['Results'] = df['Results'].replace('Fail',0)\n",
    "df['Results'] = df['Results'].replace('Pass',1)\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dfnp = np.asarray(df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dfnpX = df.drop('Results',axis=1) \n",
    "dfnpy = df[\"Results\"]\n",
    "dfnpXx = np.asarray(dfnpX)\n",
    "dfnpyy = np.asarray(dfnpy)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Splitting the data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#MinMaxScaler(feature_range = (0, 1)) will transform each value in the column proportionally within the range [0,1]. Use this as the first scaler choice to transform a feature, as it will preserve the shape of the dataset (no distortion).\n",
    "\n",
    "#StandardScaler() will transform each value in the column to range about the mean 0 and standard deviation 1, ie, each value will be normalised by subtracting the mean and dividing by standard deviation. Use StandardScaler if you know the data distribution is normal.\n",
    "\n",
    "#If there are outliers, use RobustScaler(). Alternatively you could remove the outliers and use either of the above 2 scalers (choice depends on whether data is normally distributed)\n",
    "\n",
    "#Since all the datapoints in this dataset are binary any scaler would be a good fit for our machine learning model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.model_selection import cross_val_score\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from mlxtend.plotting import plot_decision_regions\n",
    "from sklearn.datasets import load_iris\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.svm import SVC, LinearSVC\n",
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "%matplotlib inline\n",
    "\n",
    "from mlxtend.plotting import plot_decision_regions\n",
    "from sklearn.preprocessing import MinMaxScaler\n",
    "\n",
    "from sklearn.model_selection import train_test_split\n",
    "import matplotlib.gridspec as gridspec\n",
    "import itertools\n",
    "\n",
    "import numpy as np\n",
    "X = df.drop('Results',axis=1)  \n",
    "y = df[\"Results\"]\n",
    "X_train_org, X_test_org, y_train, y_test = train_test_split(X, y, random_state = 0)\n",
    "\n",
    "scaler = MinMaxScaler()\n",
    "scaler.fit(X_train_org) # Find the min and max of each column #Fit is for training dataset\n",
    "X_train = scaler.fit_transform(X_train_org) #transforms X_train_org to X_train with all valus between 0 and 1\n",
    "X_test = scaler.transform(X_test_org)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Evaluation "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We use Grid search and Cross validation to evaluate the best parameters for this dataset "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Grid search(Both Naive and grid search with Cross validation)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.svm import SVC\n",
    "\n",
    "X = df.drop('Results',axis=1)  \n",
    "y = df[\"Results\"]\n",
    "# split data into train+validation set and test set\n",
    "X_trainval, X_test, y_trainval, y_test = train_test_split(X, y, random_state=0)\n",
    "\n",
    "# split train+validation set into training and validation sets\n",
    "X_train, X_valid, y_train, y_valid = train_test_split(X_trainval, y_trainval, random_state=1)\n",
    "\n",
    "print(\"Size of training set: {}   size of validation set: {}   size of test set:\"\n",
    "      \" {}\\n\".format(X_train.shape[0], X_valid.shape[0], X_test.shape[0]))\n",
    "\n",
    "best_score = 0\n",
    "\n",
    "for gamma in [0.001, 0.01, 0.1, 1, 10, 100]:\n",
    "    for C in [0.001, 0.01, 0.1, 1, 10, 100]:\n",
    "        # for each combination of parameters, train an SVC\n",
    "        svm = SVC(gamma=gamma, C=C)\n",
    "        svm.fit(X_train, y_train)\n",
    "        # evaluate the SVC on the validation set\n",
    "        score = svm.score(X_valid, y_valid)\n",
    "        # if we got a better score, store the score and parameters\n",
    "        if score > best_score:\n",
    "            best_score = score\n",
    "            best_parameters = {'C': C, 'gamma': gamma}\n",
    "\n",
    "# rebuild a model on the combined training and validation set,\n",
    "# and evaluate it on the test set\n",
    "svm = SVC(**best_parameters)\n",
    "svm.fit(X_trainval, y_trainval)\n",
    "test_score = svm.score(X_test, y_test)\n",
    "print(\"Best score on validation set: {:.2f}\".format(best_score))\n",
    "print(\"Best parameters: \", best_parameters)\n",
    "print(\"Test set score with best parameters: {:.2f}\".format(test_score))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "\n",
    "for gamma in [0.001, 0.01, 0.1, 1, 10, 100]:\n",
    "    for C in [0.001, 0.01, 0.1, 1, 10, 100]:\n",
    "        # for each combination of parameters,\n",
    "        # train an SVC\n",
    "        svm = SVC(gamma=gamma, C=C)\n",
    "        # perform cross-validation\n",
    "        scores = cross_val_score(svm, X_trainval, y_trainval, cv=5)\n",
    "        # compute mean cross-validation accuracy\n",
    "        score = np.mean(scores)\n",
    "        # if we got a better score, store the score and parameters\n",
    "        if score > best_score:\n",
    "            best_score = score\n",
    "            best_parameters = {'C': C, 'gamma': gamma}\n",
    "            \n",
    "# rebuild a model on the combined training and validation set\n",
    "svm = SVC(**best_parameters)\n",
    "svm.fit(X_trainval, y_trainval)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.svm import SVC\n",
    "from sklearn.model_selection import train_test_split\n",
    "X = df.drop('Results',axis=1)  \n",
    "y = df[\"Results\"]\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, random_state = 0)\n",
    "\n",
    "print(\"Size of training set: {}   size of test set: {}\".format(X_train.shape[0], X_test.shape[0]))\n",
    "\n",
    "best_score = 0\n",
    "\n",
    "for gamma in [0.001, 0.01, 0.1, 1, 10, 100]:\n",
    "    for C in [0.001, 0.01, 0.1, 1, 10, 100]:\n",
    "        # for each combination of parameters, train an SVC\n",
    "        svm = SVC(gamma=gamma, C=C)\n",
    "        svm.fit(X_train, y_train)\n",
    "        # evaluate the SVC on the test set\n",
    "        score = svm.score(X_test, y_test)\n",
    "        # if we got a better score, store the score and parameters\n",
    "        if score > best_score:\n",
    "            best_score = score\n",
    "            best_parameters = {'C': C, 'gamma': gamma}\n",
    "\n",
    "print(\"Best score: {:.2f}\".format(best_score))\n",
    "print(\"Best parameters: {}\".format(best_parameters))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "param_grid = {'C': [0.001, 0.01, 0.1, 1, 10, 100],\n",
    "              'gamma': [0.001, 0.01, 0.1, 1, 10, 100]}\n",
    "print(\"Parameter grid:\\n{}\".format(param_grid))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import GridSearchCV\n",
    "from sklearn.svm import SVC\n",
    "grid_search = GridSearchCV(SVC(), param_grid, cv=5, return_train_score=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train, X_test, y_train, y_test = train_test_split(X, y, random_state=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "grid_search.fit(X_train, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"Best parameters: {}\".format(grid_search.best_params_))\n",
    "print(\"Best cross-validation score: {:.2f}\".format(grid_search.best_score_))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "# convert to DataFrame\n",
    "results = pd.DataFrame(grid_search.cv_results_)\n",
    "# show the first 5 rows\n",
    "display(results.head())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%matplotlib notebook\n",
    "import mglearn\n",
    "scores = np.array(results.mean_test_score).reshape(6, 6)\n",
    "\n",
    "# plot the mean cross-validation scoresfrom sklearn.model_selection import train_test_split\n",
    "mglearn.tools.heatmap(scores, xlabel='gamma', xticklabels=param_grid['gamma'], ylabel='C', yticklabels=param_grid['C'], cmap=\"viridis\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Grid search for decision tree models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "from sklearn.model_selection import train_test_split\n",
    "X = df.drop('Results',axis=1)  \n",
    "y = df[\"Results\"]\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, random_state = 0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dtc = DecisionTreeClassifier(random_state=0)\n",
    "# 2. Fit\n",
    "dtc.fit(X_train, y_train)\n",
    "\n",
    "# 3. Predict, there're 4 features in the iris dataset\n",
    "y_pred_class = dtc.predict(X_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn import metrics\n",
    "# Accuracy\n",
    "metrics.accuracy_score(y_test, y_pred_class)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 1. Instantiate with min_samples_split = 50\n",
    "dtc = DecisionTreeClassifier(min_samples_split=4, random_state=0)\n",
    "\n",
    "# 2. Fit\n",
    "dtc.fit(X_train, y_train)\n",
    "\n",
    "# 3. Predict, there're 4 features in the iris dataset\n",
    "y_pred_class = dtc.predict(X_test)\n",
    "\n",
    "# Accuracy\n",
    "metrics.accuracy_score(y_test, y_pred_class)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "    # Import\n",
    "    from sklearn.model_selection import GridSearchCV\n",
    "    import matplotlib.gridspec as gridspec\n",
    "\n",
    "    # Define the parameter values that should be searched\n",
    "    sample_split_range = list(range(2, 50))\n",
    "\n",
    "    # Create a parameter grid: map the parameter names to the values that should be searched\n",
    "    # Simply a python dictionary\n",
    "    # Key: parameter name\n",
    "    # Value: list of values that should be searched for that parameter\n",
    "    # Single key-value pair for param_grid\n",
    "    param_grid = dict(min_samples_split=sample_split_range)\n",
    "    \n",
    "\n",
    "    # instantiate the grid\n",
    "    grid = GridSearchCV(dtc, param_grid, cv=10, scoring='accuracy')\n",
    "\n",
    "    # fit the grid with data\n",
    "    grid.fit(X_train, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# examine the best model\n",
    "\n",
    "# Single best score achieved across all params (min_samples_split)\n",
    "print( grid.best_score_)\n",
    "\n",
    "# Dictionary containing the parameters (min_samples_split) used to generate that score\n",
    "print(grid.best_params_)\n",
    "\n",
    "# Actual model object fit with those best parameters\n",
    "# Shows default parameters that we did not specify\n",
    "print(grid.best_estimator_)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Cross Validation for logistic regression"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import cross_val_score\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from mlxtend.plotting import plot_decision_regions\n",
    "from sklearn.datasets import load_iris\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.svm import SVC, LinearSVC\n",
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "import matplotlib.gridspec as gridspec\n",
    "import itertools\n",
    "import numpy as np\n",
    "\n",
    "X = df.drop('Results',axis=1)  \n",
    "y = df[\"Results\"]\n",
    "X_train_org, X_test_org, y_train, y_test = train_test_split(X, y, random_state = 0)\n",
    "\n",
    "scaler = StandardScaler()\n",
    "scaler.fit(X_train_org)\n",
    "X_train = scaler.transform(X_train_org)\n",
    "X_test = scaler.transform(X_test_org)\n",
    "\n",
    "logreg = LogisticRegression()\n",
    "\n",
    "scores = cross_val_score(logreg, X_train, y_train,cv=10)\n",
    "print(\"Cross-validation scores: {}\".format(scores))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"Average cross-validation score: {:.2f}\".format(scores.mean()))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Cross Validation for K Neighbors Classification"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import cross_val_score\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "\n",
    "knn = KNeighborsClassifier()\n",
    "\n",
    "\n",
    "scores = cross_val_score(knn, X_train, y_train,cv=10)\n",
    "print(\"Cross-validation scores: {}\".format(scores))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"Average cross-validation score: {:.2f}\".format(scores.mean()))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Cross Validation for Linear SVC"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import cross_val_score\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "\n",
    "\n",
    "logreg = LogisticRegression()\n",
    "knn = KNeighborsClassifier()\n",
    "clf = LinearSVC()\n",
    "\n",
    "scores = cross_val_score(clf, X_train, y_train,cv=10)\n",
    "print(\"Cross-validation scores: {}\".format(scores))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"Average cross-validation score: {:.2f}\".format(scores.mean()))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Cross Validation for Linear kernel SVC"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import cross_val_score\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "\n",
    "clf = SVC(kernel='linear',C = 0.001)\n",
    "\n",
    "scores = cross_val_score(clf, X_train, y_train,cv=15)\n",
    "print(\"Cross-validation scores: {}\".format(scores))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"Average cross-validation score: {:.4f}\".format(scores.mean()))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Cross Validation for rbf kernel SVC"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import cross_val_score\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "\n",
    "clf = SVC(kernel ='rbf',C = 0.001,gamma = 0.001)\n",
    "\n",
    "scores = cross_val_score(clf, X_train, y_train,cv=15)\n",
    "print(\"Cross-validation scores: {}\".format(scores))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"Average cross-validation score: {:.4f}\".format(scores.mean()))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Cross Validation for poly kernel SVC"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import cross_val_score\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "\n",
    "clf = SVC(kernel ='poly', C = 0.001, gamma = 0.001, degree = 3, coef0=0.0)\n",
    "                \n",
    "scores = cross_val_score(clf, X_train, y_train,cv=10)\n",
    "print(\"Cross-validation scores: {}\".format(scores))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"Average cross-validation score: {:.4f}\".format(scores.mean()))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Cross Validation for Decision tree Classifier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.tree import DecisionTreeClassifier\n",
    "from sklearn.model_selection import cross_val_score\n",
    "\n",
    "dtc = DecisionTreeClassifier(min_samples_split=49, random_state=0)\n",
    "dtc.fit(X_train, y_train)\n",
    "\n",
    "scores = cross_val_score(dtc, X_train, y_train,cv=10)\n",
    "print(\"Cross-validation scores: {}\".format(scores))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"Average cross-validation score: {:.4f}\".format(scores.mean()))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## K nearest neighbor Classifier "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%matplotlib inline\n",
    "\n",
    "from mlxtend.plotting import plot_decision_regions\n",
    "from sklearn.preprocessing import MinMaxScaler\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "X = df.drop('Results',axis=1)  \n",
    "y = df[\"Results\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train_org, X_test_org, y_train, y_test = train_test_split(X, y, random_state = 0)\n",
    "\n",
    "scaler = MinMaxScaler()\n",
    "scaler.fit(X_train_org) # Find the min and max of each column #Fit is for training dataset\n",
    "X_train = scaler.fit_transform(X_train_org) #transforms X_train_org to X_train with all valus between 0 and 1\n",
    "# = scaler.fit_transform(X_train_org)\n",
    "X_test = scaler.transform(X_test_org)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "\n",
    "train_score_array = []\n",
    "test_score_array = []\n",
    "\n",
    "for k in range(1,20):\n",
    "    knn = KNeighborsClassifier(k)\n",
    "    knn.fit(X_train, y_train)\n",
    "    train_score_array.append(knn.score(X_train, y_train))\n",
    "    test_score_array.append(knn.score(X_test, y_test))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "x_axis = range(1,20)\n",
    "%matplotlib inline\n",
    "plt.plot(x_axis, train_score_array, label = 'Train Score', c = 'g')\n",
    "plt.plot(x_axis, test_score_array, label = 'Test Score', c='b')\n",
    "plt.xlabel('k')\n",
    "plt.ylabel('Accuracy')\n",
    "plt.legend()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "It seems k = 6 is the best parameter for knn model. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "knn = KNeighborsClassifier(6)\n",
    "knn.fit(X_train, y_train)\n",
    "print('Train score: {:.5f}'.format(knn.score(X_train, y_train)))\n",
    "print('Test score: {:.5f}'.format(knn.score(X_test, y_test)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Logistic Regression"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.to_numpy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.linear_model import LogisticRegression\n",
    "\n",
    "c_range = [0.001, 0.01, 0.1, 1, 10, 100, 1000]\n",
    "train_score_l1 = []\n",
    "train_score_l2 = []\n",
    "test_score_l1 = []\n",
    "test_score_l2 = []\n",
    "\n",
    "for c in c_range:\n",
    "    log_l1 = LogisticRegression(penalty = 'l1', C = c)\n",
    "    log_l2 = LogisticRegression(penalty = 'l2', C = c)\n",
    "    log_l1.fit(X_train, y_train)\n",
    "    log_l2.fit(X_train, y_train)\n",
    "    train_score_l1.append(log_l1.score(X_train, y_train))\n",
    "    train_score_l2.append(log_l2.score(X_train, y_train))\n",
    "    test_score_l1.append(log_l1.score(X_test, y_test))\n",
    "    test_score_l2.append(log_l2.score(X_test, y_test))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%matplotlib inline\n",
    "\n",
    "plt.plot(c_range, train_score_l1, label = 'Train score, penalty= l1')\n",
    "plt.plot(c_range, test_score_l1, label = 'Test score, penalty = l1')\n",
    "plt.plot(c_range, train_score_l2, label = 'Train score, penalty = l2')\n",
    "plt.plot(c_range, test_score_l2, label = 'Test score, penalty = l2')\n",
    "plt.legend()\n",
    "plt.xlabel('Regularization parameter: C')\n",
    "plt.ylabel('Accuracy')\n",
    "plt.xscale('log')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "It seems C = 10^-2 is the best parameter for Logistic model. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%matplotlib inline\n",
    "\n",
    "from mlxtend.plotting import plot_decision_regions\n",
    "\n",
    "X = df.drop('Results',axis=1)  \n",
    "y = df[\"Results\"]\n",
    "X_train_org, X_test_org, y_train, y_test = train_test_split(X, y, random_state = 1)\n",
    "\n",
    "scaler = MinMaxScaler()\n",
    "scaler.fit(X_train_org) # Find the min and max of each column #Fit is for training dataset\n",
    "X_train = scaler.fit_transform(X_train_org) #transforms X_train_org to X_train with all valus between 0 and 1\n",
    "# = scaler.fit_transform(X_train_org)\n",
    "X_test = scaler.transform(X_test_org)\n",
    "\n",
    "lreg = LogisticRegression(C=0.01)\n",
    "lreg.fit(X_train, y_train) \n",
    "\n",
    "print('Train score: {:.4f}'.format(lreg.score(X_train, y_train)))\n",
    "print('Test score: {:.4f}'.format(lreg.score(X_test, y_test)))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Linear Support Vector Machine"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%matplotlib inline\n",
    "\n",
    "from mlxtend.plotting import plot_decision_regions\n",
    "from sklearn.svm import LinearSVC\n",
    "import numpy as np\n",
    "random_state = 0\n",
    "X = df.drop('Results',axis=1)  \n",
    "y = df[\"Results\"]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "clf = LinearSVC()\n",
    "clf.fit(X_train,y_train)\n",
    "\n",
    "print('Train score: {:.4f}'.format(clf.score(X_train, y_train)))\n",
    "print('Test score: {:.4f}'.format(clf.score(X_test, y_test)))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## SVC with kernel trick"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%matplotlib inline\n",
    "\n",
    "from mlxtend.plotting import plot_decision_regions\n",
    "from sklearn.datasets import load_iris\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.svm import SVC, LinearSVC\n",
    "import matplotlib.gridspec as gridspec\n",
    "import itertools\n",
    "import numpy as np\n",
    "X = df.drop('Results',axis=1)  \n",
    "y = df[\"Results\"]\n",
    "X_train_org, X_test_org, y_train, y_test = train_test_split(X, y, random_state = 0)\n",
    "\n",
    "scaler = MinMaxScaler()\n",
    "scaler.fit(X_train_org) # Find the min and max of each column #Fit is for training dataset\n",
    "X_train = scaler.fit_transform(X_train_org) #transforms X_train_org to X_train with all valus between 0 and 1\n",
    "# = scaler.fit_transform(X_train_org)\n",
    "X_test = scaler.transform(X_test_org)\n",
    "\n",
    "clf2 = SVC(kernel='linear', C=0.001)\n",
    "clf2.fit(X_train, y_train)\n",
    "print('Train score: {:.4f}'.format(clf2.score(X_train, y_train)))\n",
    "print('Test score: {:.4f}'.format(clf2.score(X_test, y_test)))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%matplotlib inline\n",
    "\n",
    "from mlxtend.plotting import plot_decision_regions\n",
    "from sklearn.datasets import load_iris\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.svm import SVC, LinearSVC\n",
    "import matplotlib.gridspec as gridspec\n",
    "import itertools\n",
    "import numpy as np\n",
    "\n",
    "C = 1\n",
    "clf3 = SVC(kernel='rbf', gamma=0.001, C=0.001)\n",
    "clf3.fit(X_train, y_train)\n",
    "\n",
    "\n",
    "print('Train score: {:.4f}'.format(clf3.score(X_train, y_train)))\n",
    "print('Test score: {:.4f}'.format(clf3.score(X_test, y_test)))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%matplotlib inline\n",
    "\n",
    "from mlxtend.plotting import plot_decision_regions\n",
    "from sklearn.datasets import load_iris\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.svm import SVC, LinearSVC\n",
    "import matplotlib.gridspec as gridspec\n",
    "import itertools\n",
    "import numpy as np\n",
    "\n",
    "C = 1\n",
    "clf = SVC(kernel='poly',degree=3, gamma=0.001, C=0.001)\n",
    "clf.fit(X_train, y_train)\n",
    "\n",
    "\n",
    "print('Train score: {:.4f}'.format(clf.score(X_train, y_train)))\n",
    "print('Test score: {:.4f}'.format(clf.score(X_test, y_test)))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Decision tree Classifier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%matplotlib notebook\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "from sklearn.datasets import load_breast_cancer\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "X = df.drop('Results',axis=1)  \n",
    "y = df[\"Results\"]\n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, random_state = 0)\n",
    "\n",
    "dtree = DecisionTreeClassifier(min_samples_split = 49,max_depth=None,random_state=0)\n",
    "\n",
    "dtree.fit(X_train, y_train)\n",
    "print(\"Accuracy on training set: {:.4f}\".format(dtree.score(X_train, y_train)))\n",
    "print(\"Accuracy on test set: {:.4f}\".format(dtree.score(X_test, y_test)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We see that the Decision tree classifier and K neighbour Classifier give the best result for this dataset \n",
    "\n",
    "### <font color='Green'>K neighbour classification\n",
    " Train score: 0.79036\n",
    " Test score: 0.76376\n",
    "### <font color='blue'>Logistic Regression\n",
    " Train score: 0.7827\n",
    " Test score: 0.7798\n",
    "### <font color='blue'>Linear SVC\n",
    " Train score: 0.7827\n",
    " Test score: 0.7798\n",
    "### <font color='blue'>Kernel linear SVC\n",
    " Train score: 0.7766\n",
    " Test score: 0.7982\n",
    "### <font color='blue'>Kernel rbf SVC\n",
    " Train score: 0.7766\n",
    " Test score: 0.7982\n",
    "### <font color='blue'>Kernel Poly SVC\n",
    " Train score: 0.7766\n",
    " Test score: 0.7982\n",
    "### <font color='Green'>Decision Tree Classifier:\n",
    " Accuracy on training set: 0.8103\n",
    " Accuracy on test set: 0.7683"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
